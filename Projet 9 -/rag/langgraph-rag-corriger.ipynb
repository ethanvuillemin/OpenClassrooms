{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef0cdd6",
   "metadata": {},
   "source": [
    "# LangGraph Agentic Rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e640cabe",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd7445a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistral-large-latest\n",
      "NuNTGZkdk67xWT4RcEYzCYtAHvsPpK71\n",
      "https://api.mistral.ai/v1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import operator\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "print(os.getenv(\"MODEL_NAME\"))\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))\n",
    "print(os.getenv(\"OPENAI_API_BASE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bbfc8b",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bca08ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_model = ChatOpenAI(\n",
    "    model=os.getenv(\"MODEL_NAME\"),\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"),\n",
    "    max_tokens=8000,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "\n",
    "grader_model = ChatOpenAI(\n",
    "    model=os.getenv(\"MODEL_NAME\"),\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"),\n",
    "    max_tokens=8000,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f2d46",
   "metadata": {},
   "source": [
    "# 0. Process des documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5c904f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = \"./data\"\n",
    "EMBEDDING_MODEL_PATH = \"./models/sentence_transformers\"\n",
    "LLM_MODEL_PATH = \"./models/Falcon3-3B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "543305e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code civil\n",
      "Titre préliminaire : De la publication, des effets et de l'application des\n",
      "lois en général\n",
      "Article 1\n",
      " \n",
      "Les lois et, lorsqu'ils sont publiés au Journal officiel de la République française, les actes administratifs\n",
      "entrent en vigueur à la date qu'ils fixent ou, à défaut, le lendemain de leur publication. Toutefois, l'entrée en\n",
      "vigueur de celles de leurs dispositions dont l'exécution nécessite des mesures d'application est reportée à la\n",
      "date d'entrée en vigueur de ces mesures.\n",
      " \n",
      " \n",
      "En cas d'urgence, entrent en vigueur dès leur publication les lois dont le décret de promulgation le prescrit et\n",
      "les actes administratifs pour lesquels le Gouvernement l'ordonne par une disposition spéciale.\n",
      " \n",
      " \n",
      "Les dispositions du présent article ne sont pas applicables aux actes individuels.\n",
      " \n",
      "Article 2\n",
      " \n",
      "La loi ne dispose que pour l'avenir ; elle n'a point d'effet rétroactif.\n",
      " \n",
      " \n",
      "Article 3\n",
      " \n",
      "Les lois de police et de sûreté obligent tous ceux qui habitent le territoire.\n",
      " \n",
      " \n",
      "Les immeubles, même ceux possédés par des étrangers, sont régis par la loi française.\n",
      " \n",
      " \n",
      "Les lois concernant l'état et la capacité des personnes régissent les Français, même résidant en pays étranger.\n",
      " \n",
      "Article 4\n",
      " \n",
      "Le juge qui refusera de juger, sous prétexte du silence, de l'obscurité ou de l'insuffisance de la loi, pourra être\n",
      "poursuivi comme coupable de déni de justice.\n",
      " \n",
      " \n",
      "Article 5\n",
      "Code civil - Dernière modification le 14 septembre 2024 - Document généré le 10 février 2025\n"
     ]
    }
   ],
   "source": [
    "# Load les pdfs\n",
    "def load_pdf_from_diretory(dir_path: str):\n",
    "    \"\"\"Load local pdf file as Langchain document object\"\"\"\n",
    "    documents = []\n",
    "    for filename in os.listdir(dir_path):\n",
    "        loader = PyPDFLoader(os.path.join(dir_path, filename))\n",
    "        documents.extend(loader.load())\n",
    "    return documents\n",
    "\n",
    "\n",
    "docs_before_split = load_pdf_from_diretory(PDF_PATH)\n",
    "print(docs_before_split[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c6be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_embedding_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_PATH,\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "\n",
    "# Initialiser le chunker\n",
    "text_splitter = SemanticChunker(huggingface_embedding_model)\n",
    "\n",
    "# Decoupe des data en chunks\n",
    "docs_after_split = text_splitter.split_documents(docs_before_split)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs_after_split, huggingface_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ec7c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# Créer l'outil de récupération\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"retrieve_blog_posts\",\n",
    "    description=\"Search and return information about the python documentation.\",\n",
    ")\n",
    "\n",
    "# Lier l'outil au modèle\n",
    "model_with_tools = response_model.bind_tools([retriever_tool])\n",
    "\n",
    "\n",
    "result = retriever_tool.invoke({\"query\": \"python command to create a virtual env\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f57833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des types et fonctions de base\n",
    "class MessagesState(Dict):\n",
    "    messages: List[Union[HumanMessage, AIMessage, ToolMessage]]\n",
    "\n",
    "def safe_get_messages(state: MessagesState) -> List[Union[HumanMessage, AIMessage, ToolMessage]]:\n",
    "    \"\"\"Récupère les messages de manière sûre, avec des valeurs par défaut si nécessaire\"\"\"\n",
    "    return state.get(\"messages\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f97387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state: MessagesState) -> str:\n",
    "    \"\"\"Évalue la pertinence des documents récupérés de manière plus robuste\"\"\"\n",
    "    messages = safe_get_messages(state)\n",
    "\n",
    "    # Vérifier qu'il y a assez de messages\n",
    "    if len(messages) < 2:\n",
    "        print(\"Warning: Not enough messages to evaluate relevance\")\n",
    "        return \"rewrite_question\"\n",
    "\n",
    "    try:\n",
    "        question = messages[0].content\n",
    "        context = messages[-1].content\n",
    "\n",
    "        # Vérifier que le contenu existe\n",
    "        if not question or not context:\n",
    "            return \"rewrite_question\"\n",
    "\n",
    "        # Créer le prompt pour l'évaluation\n",
    "        prompt = f\"\"\"\n",
    "        Évaluez si ce document est pertinent pour répondre à la question sur les environnements virtuels Python.\n",
    "\n",
    "        Question: {question}\n",
    "        Document: {context}\n",
    "\n",
    "        Répondez uniquement par 'yes' si le document est pertinent, ou 'no' s'il ne l'est pas.\n",
    "        \"\"\"\n",
    "\n",
    "        # Utiliser le modèle pour évaluer\n",
    "        evaluation = grader_model.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "        # Post-traitement pour s'assurer de la pertinence\n",
    "        if evaluation and hasattr(evaluation, 'content') and \"yes\" in evaluation.content.lower():\n",
    "            return \"generate_answer\"\n",
    "        else:\n",
    "            return \"rewrite_question\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in grade_documents: {str(e)}\")\n",
    "        return \"rewrite_question\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6662a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b6fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_query_or_response(state: MessagesState) -> MessagesState:\n",
    "    \"\"\"Version plus robuste de la génération de réponse\"\"\"\n",
    "    messages = safe_get_messages(state)\n",
    "\n",
    "    if not messages:\n",
    "        return {\"messages\": [AIMessage(content=\"Je n'ai pas reçu de question valide.\")]}\n",
    "\n",
    "    try:\n",
    "        # Convertir les messages en format attendu par le modèle\n",
    "        input_messages = []\n",
    "        for msg in messages:\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                input_messages.append({\"role\": \"user\", \"content\": msg.content})\n",
    "            elif isinstance(msg, AIMessage):\n",
    "                tool_calls = []\n",
    "                if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                    tool_calls = msg.tool_calls\n",
    "                input_messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": msg.content,\n",
    "                    \"tool_calls\": tool_calls\n",
    "                })\n",
    "            elif isinstance(msg, ToolMessage):\n",
    "                input_messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": msg.content,\n",
    "                    \"tool_call_id\": msg.tool_call_id\n",
    "                })\n",
    "\n",
    "        # Appeler le modèle avec les outils\n",
    "        response = response_model.bind_tools([retriever_tool]).invoke(input_messages)\n",
    "\n",
    "        # Traiter la réponse\n",
    "        if isinstance(response, dict):\n",
    "            content = response.get(\"content\", \"\")\n",
    "            tool_calls = response.get(\"tool_calls\", [])\n",
    "            new_message = AIMessage(content=content)\n",
    "            if tool_calls:\n",
    "                new_message.tool_calls = tool_calls\n",
    "            new_messages = messages + [new_message]\n",
    "        else:\n",
    "            new_messages = messages + [AIMessage(content=str(response))]\n",
    "\n",
    "        return {\"messages\": new_messages}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_query_or_response: {str(e)}\")\n",
    "        return {\"messages\": messages + [AIMessage(content=\"Une erreur est survenue lors du traitement de votre demande.\")]}\n",
    "\n",
    "# Configuration du graphe avec gestion d'erreurs\n",
    "try:\n",
    "    workflow = StateGraph(MessagesState)\n",
    "\n",
    "    # Ajouter les nœuds avec vérification\n",
    "    workflow.add_node(\"generate_query_or_response\", generate_query_or_response)\n",
    "    workflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\n",
    "    workflow.add_node(\"rewrite_question\", rewrite_question)\n",
    "    workflow.add_node(\"generate_answer\", generate_answer)\n",
    "\n",
    "    # Définir les arêtes avec vérification\n",
    "    workflow.add_edge(START, \"generate_query_or_response\")\n",
    "\n",
    "    def should_use_tools(state: MessagesState) -> str:\n",
    "        \"\"\"Fonction conditionnelle plus robuste\"\"\"\n",
    "        messages = safe_get_messages(state)\n",
    "        if not messages:\n",
    "            return END\n",
    "\n",
    "        last_message = messages[-1]\n",
    "        if isinstance(last_message, AIMessage) and hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "            return \"retrieve\"\n",
    "        return END\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"generate_query_or_response\",\n",
    "        should_use_tools,\n",
    "        {\n",
    "            \"retrieve\": \"retrieve\",\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"retrieve\",\n",
    "        grade_documents,\n",
    "        {\n",
    "            \"generate_answer\": \"generate_answer\",\n",
    "            \"rewrite_question\": \"rewrite_question\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    workflow.add_edge(\"generate_answer\", END)\n",
    "    workflow.add_edge(\"rewrite_question\", \"generate_query_or_response\")\n",
    "\n",
    "    # Compiler le graphe\n",
    "    graph = workflow.compile()\n",
    "\n",
    "    # Exécuter avec une entrée valide\n",
    "    initial_input = {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"What is a virtual environment in Python?\")\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    result = graph.invoke(initial_input)\n",
    "\n",
    "    # Afficher le résultat de manière sûre\n",
    "    if result and isinstance(result, dict) and \"messages\" in result:\n",
    "        final_messages = result[\"messages\"]\n",
    "        if final_messages:\n",
    "            final_message = final_messages[-1]\n",
    "            if isinstance(final_message, (AIMessage, HumanMessage, ToolMessage)):\n",
    "                print(\"Final response:\", final_message.content)\n",
    "            else:\n",
    "                print(\"Final response:\", str(final_message))\n",
    "        else:\n",
    "            print(\"No messages in the final result\")\n",
    "    else:\n",
    "        print(\"Invalid final result format\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in workflow setup: {str(e)}\")\n",
    "    # Afficher des informations de débogage supplémentaires\n",
    "    print(\"Debug info:\")\n",
    "    print(f\"Type of error: {type(e)}\")\n",
    "    if hasattr(e, 'message'):\n",
    "        print(f\"Error message: {e.message}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
