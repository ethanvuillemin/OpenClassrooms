{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e0a6aa",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Build Agentic Rag with langgraph](#toc0_)\n",
    "https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6676fb4f",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Build Agentic Rag with langgraph](#toc1_)    \n",
    "- [Process Documents](#toc2_)    \n",
    "- [Create the retrieval tools](#toc3_)    \n",
    "  - [Store in a vector store (FAISS)](#toc3_1_)    \n",
    "- [Grade documents](#toc4_)    \n",
    "- [Rewrite question](#toc5_)    \n",
    "      - [Test de reecriture de la question apres une reponse non concluante du tool](#toc5_1_1_1_)    \n",
    "- [Generate an answer](#toc6_)    \n",
    "      - [Test de la generation de reponse](#toc6_1_1_1_)    \n",
    "- [Assemble the graph](#toc7_)    \n",
    "- [Run the agentic RAG](#toc8_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cc99a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture --no-stderr\n",
    "# %pip install -U --quiet langgraph \"langchain[openai]\" langchain-community langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2256bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import MessagesState\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PDF_PATH = \"./data\"\n",
    "EMBEDDING_MODEL_PATH = \"./models/sentence_transformers\"\n",
    "SAMPLE_QUERY= \"Quels sont les principaux composants du modèle Transformer ?\"\n",
    "SAMPLE_YES = \"Les principaux composants du modèle Transformer sont l'encodeur, le décodeur, et le mécanisme d'attention.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babd5e8c",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Process Documents](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "167b2afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Introduction\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [38, 24, 15].\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "significant improvements in computational efficiency through factorization tricks [21] and conditional\n",
      "computation [32], while also improving model performance in case of the latter. The fundamental\n",
      "constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\n",
      "of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\n",
      "sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n",
      "[10], consuming the previously generated symbols as additional input when generating the next.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def load_pdf_from_directory(dir_path: str):\n",
    "    \"\"\"Load local pdf files as Langchain document objects\"\"\"\n",
    "    documents = []\n",
    "    for filename in os.listdir(dir_path):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        if os.path.isfile(file_path) and filename.lower().endswith('.pdf'):\n",
    "            try:\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                documents.extend(loader.load())\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {filename}: {e}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "docs_before_split = load_pdf_from_directory(PDF_PATH)\n",
    "print(docs_before_split[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb188b5",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Create the retrieval tools](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "07135eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_embedding_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_PATH,\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "\n",
    "\n",
    "# Initialiser le chunker\n",
    "text_splitter = SemanticChunker(huggingface_embedding_model)\n",
    "\n",
    "# Decoupe des data en chunks\n",
    "docs_after_split = text_splitter.split_documents(docs_before_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b050d3d",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Store in a vector store (FAISS)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fa05e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(docs_after_split, huggingface_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f2df42b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cb43abf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\n",
      "model. All metrics are on the English-to-German translation development set, newstest2013. Listed\n",
      "perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\n",
      "per-word perplexities. N d model dff h d k dv Pdrop ϵls\n",
      "train PPL BLEU params\n",
      "steps (dev) (dev) ×106\n",
      "base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n",
      "(A)\n",
      "1 512 512 5.29 24.9\n",
      "4 128 128 5.00 25.5\n",
      "16 32 32 4.91 25.8\n",
      "32 16 16 5.01 25.4\n",
      "(B) 16 5.16 25.1 58\n",
      "32 5.01 25.4 60\n",
      "(C)\n",
      "2 6.11 23.7 36\n",
      "4 5.19 25.3 50\n",
      "8 4.88 25.5 80\n",
      "256 32 32 5.75 24.5 28\n",
      "1024 128 128 4.66 26.0 168\n",
      "1024 5.12 25.4 53\n",
      "4096 4.75 26.2 90\n",
      "(D)\n",
      "0.0 5.77 24.6\n",
      "0.2 4.95 25.5\n",
      "0.0 4.67 25.3\n",
      "0.2 5.47 25.7\n",
      "(E) positional embedding instead of sinusoids 4.92 25.7\n",
      "big 6 1024 4096 16 0.3 300K 4.33 26.4 213\n",
      "development set, newstest2013. We used beam search as described in the previous section, but no\n",
      "checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\n",
      "keeping the amount of computation constant, as described in Section 3.2.2. While single-head\n",
      "attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\n",
      "suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
      "function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\n",
      "bigger models are better, and dropout is very helpful in avoiding over-fitting.\n",
      "\n",
      "Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively. 3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [11] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1].\n",
      "\n",
      "For the big models, we averaged the last 20 checkpoints. We\n",
      "used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\n",
      "were chosen after experimentation on the development set. We set the maximum output length during\n",
      "inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model\n",
      "architectures from the literature. We estimate the number of floating point operations used to train a\n",
      "model by multiplying the training time, the number of GPUs used, and an estimate of the sustained\n",
      "single-precision floating-point capacity of each GPU 5. 6.2 Model Variations\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model\n",
      "in different ways, measuring the change in performance on English-to-German translation on the\n",
      "5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively. 8\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# Initialiser le modèle\n",
    "response_model = ChatOpenAI(\n",
    "    model=os.getenv(\"MODEL_NAME\"),\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"),\n",
    "    max_tokens=8000,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Créer l'outil de récupération\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"retrieve_blog_posts\",\n",
    "    description=\"Search and return information about attention is all you need paper.\",\n",
    ")\n",
    "\n",
    "# Lier l'outil au modèle\n",
    "model_with_tools = response_model.bind_tools([retriever_tool])\n",
    "\n",
    "\n",
    "result = retriever_tool.invoke({\"query\": SAMPLE_QUERY})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bd5e1204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response: content=\"<think>\\nOkay, the user is asking about the main components of the Transformer model. Let me recall what I know. The Transformer is a type of neural network used in natural language processing. It's different from RNNs because it uses self-attention mechanisms. The key components include the encoder and decoder, which process the input sequence, and the attention mechanism, which helps in finding relevant parts of the input. Also, there's the attention layer that allows the model to focus on specific parts of the input. I should make sure to mention these components clearly. Let me structure the answer to cover each part and explain how they work together.\\n</think>\\n\\nLes principaux composants du modèle Transformer sont :\\n\\n1. **Encoder** :  \\n   - Processe les séquences d'input en utilisant des **matrices de transition** (W) pour convertir les données en vecteurs d'attention.  \\n   - Intégre des **matrices de transition** (W) pour transformer les données en vecteurs d'attention.\\n\\n2. **Attention (Self-Attention)** :  \\n   - Permet de **chercher des motifs** dans l'input par des **matrices d'attention** (Q, K, V).  \\n   - Utilise des **matrices de transition** (W) pour transformer les vecteurs d'attention en nouvelles séquences.\\n\\n3. **Decoder** :  \\n   - Transforme les vecteurs d'attention en résultats finaux (output) en utilisant des **matrices de transition** (W) et des **matrices de transition** (W).\\n\\n4. **Attention Layer** :  \\n   - Un layer de **self-attention** qui permet de **chercher des motifs** dans l'input, permettant une meilleure répartition des efforts dans le modèle.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 373, 'prompt_tokens': 176, 'total_tokens': 549, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': '/models/qwen3-0.6b', 'system_fingerprint': None, 'id': 'chatcmpl-3a23be789cd9496b9327fee5a7d7bcb3', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--ff7bff57-71aa-45a7-8e6b-5df12119571f-0' usage_metadata={'input_tokens': 176, 'output_tokens': 373, 'total_tokens': 549, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# Tester l'appel à l'outil\n",
    "input_message = HumanMessage(content=SAMPLE_QUERY)\n",
    "\n",
    "# Obtenir la réponse du modèle\n",
    "response = model_with_tools.invoke([input_message])\n",
    "print(\"Model response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "153f562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def generate_query_or_response(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    response = model_with_tools.invoke(messages)\n",
    "\n",
    "    if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "        tool_calls = response.tool_calls\n",
    "        for tool_call in tool_calls:\n",
    "            if tool_call['name'] == 'retrieve_blog_posts':\n",
    "                # Exécuter l'outil avec les arguments\n",
    "                tool_response = retriever_tool.invoke(tool_call['arguments'])\n",
    "                # print(\"Tool response:\", tool_response)  # Afficher la réponse de l'outil\n",
    "\n",
    "                # Ajouter la réponse de l'outil aux messages\n",
    "                tool_message = ToolMessage(content=str(tool_response), tool_call_id=tool_call['id'])\n",
    "\n",
    "                # Réinvoker le modèle avec la réponse de l'outil\n",
    "                final_response = model_with_tools.invoke(messages + [tool_message])\n",
    "\n",
    "                # Retourner l'état final avec les messages mis à jour\n",
    "                return MessagesState(messages=[final_response])\n",
    "    else:\n",
    "        # Si aucun appel d'outil n'est détecté dans les métadonnées, vérifier le contenu pour des balises <tool_call>\n",
    "        tool_call_pattern = re.compile(r'<tool_call>(.*?)</tool_call>', re.DOTALL)\n",
    "        matches = tool_call_pattern.findall(response.content)\n",
    "        if matches:\n",
    "            for match in matches:\n",
    "                try:\n",
    "                    tool_call = json.loads(match)\n",
    "                    if tool_call['name'] == 'retrieve_blog_posts':\n",
    "                        # Exécuter l'outil avec les arguments\n",
    "                        tool_response = retriever_tool.invoke(tool_call['arguments'])\n",
    "                        # print(\"Tool response:\", tool_response)  # Afficher la réponse de l'outil\n",
    "\n",
    "                        # Ajouter la réponse de l'outil aux messages\n",
    "                        tool_message = ToolMessage(content=str(tool_response), tool_call_id='manual-tool-call')\n",
    "\n",
    "                        # Réinvoker le modèle avec la réponse de l'outil\n",
    "                        final_response = model_with_tools.invoke(messages + [tool_message])\n",
    "\n",
    "                        # Retourner l'état final avec les messages mis à jour\n",
    "                        return MessagesState(messages=[final_response])\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(\"Error decoding tool call:\", e)\n",
    "        else:\n",
    "            return MessagesState(messages=[response])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be44a338",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Grade documents](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "59ad0407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# Initialiser le modèle de notation\n",
    "grader_model = ChatOpenAI(\n",
    "    model=os.getenv(\"MODEL_NAME\"),\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"),\n",
    "    max_tokens=8000,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "#Pydantic class\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Use a binary score for relevance check\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Définir le prompt de notation amélioré\n",
    "GRADE_PROMPT = (\n",
    "    \"You are a grader assessing the relevance of a retrieved document to a user question. \\n\"\n",
    "    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n",
    "    \"Here is the user question: {question} \\n\"\n",
    "    \"If the document contains keywords or semantic meaning related to the user question, grade it as relevant. \\n\"\n",
    "    \"Give a binary score 'yes' or 'no' to indicate whether the document is relevant to the question. \\n\"\n",
    "    \"Be very strict in your evaluation. Only return 'yes' if the document is clearly relevant. \\n\"\n",
    "    \"Here are some examples: \\n\"\n",
    "    \"Example 1: \\n\"\n",
    "    \"Document: Reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering. \\n\"\n",
    "    \"Question: What does Lilian Weng say about types of reward hacking? \\n\"\n",
    "    \"Score: yes \\n\"\n",
    "    \"Example 2: \\n\"\n",
    "    \"Document: Meow \\n\"\n",
    "    \"Question: What does Lilian Weng say about types of reward hacking? \\n\"\n",
    "    \"Score: no \\n\"\n",
    "    \"Now, evaluate the following document and question: \\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9aef467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state: MessagesState):\n",
    "    \"\"\"Détermine si les documents sont pertinents ou non pour répondre à la question.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
    "    response = (\n",
    "        grader_model\n",
    "        .with_structured_output(GradeDocuments)\n",
    "        .invoke([HumanMessage(content=prompt)])\n",
    "    )\n",
    "    score = response.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        return \"generate_answer\"\n",
    "    else:\n",
    "        return \"rewrite_question\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7a062acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next step for relevant document: generate_answer\n"
     ]
    }
   ],
   "source": [
    "# Exemple de cas avec un score \"yes\"\n",
    "input_yes = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=SAMPLE_QUERY),\n",
    "        ToolMessage(content=\"\", tool_call_id=\"1\", name=\"retrieve_blog_posts\", additional_kwargs={\"args\": {\"query\": \"types of reward hacking\"}}),\n",
    "        ToolMessage(content=SAMPLE_YES, tool_call_id=\"1\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Appeler la fonction grade_documents\n",
    "next_step_yes = grade_documents(input_yes)\n",
    "print(\"Next step for relevant document:\", next_step_yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1496f69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next step for irrelevant document: rewrite_question\n"
     ]
    }
   ],
   "source": [
    "# Exemple de cas avec un score \"no\"\n",
    "input_no = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=SAMPLE_QUERY),\n",
    "        ToolMessage(content=\"\", tool_call_id=\"1\", name=\"retrieve_blog_posts\", additional_kwargs={\"args\": {\"query\": \"types of reward hacking\"}}),\n",
    "        ToolMessage(content=\"blablabla\", tool_call_id=\"1\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Appeler la fonction grade_documents\n",
    "next_step_no = grade_documents(input_no)\n",
    "print(\"Next step for irrelevant document:\", next_step_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc742672",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Rewrite question](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5ca46bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWRITE_PROMPT = (\n",
    "    \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"\n",
    "    \"Here is the initial question:\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"{question}\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"Formulate an improved question:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b6db73b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_question(state: MessagesState):\n",
    "    \"\"\"Réécrit/Reformule la question originale de l'utilisateur.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    prompt = REWRITE_PROMPT.format(question=question)\n",
    "    response = response_model.invoke([HumanMessage(content=prompt)])\n",
    "    return {\"messages\": [HumanMessage(content=response.content)]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02af54a",
   "metadata": {},
   "source": [
    "#### <a id='toc5_1_1_1_'></a>[Test de reecriture de la question apres une reponse non concluante du tool](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "436e1fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user wants me to formulate an improved question based on the initial input. The original question is \"Quels sont les principaux composants du modèle Transformer ?\" which translates to \"What are the main components of the Transformer model?\" \n",
      "\n",
      "First, I need to understand the user's intent. They probably want a more specific or better phrased question. The original question is straightforward but maybe too general. Let me think about possible improvements.\n",
      "\n",
      "The user might be looking for a question that invites a detailed explanation of the components. Maybe they want to ask about the key parts of the Transformer model. So, instead of \"What are the main components,\" perhaps a more precise phrasing would be better. For example, \"What are the primary components of the Transformer model?\" or \"What are the main building blocks of the Transformer model?\" \n",
      "\n",
      "I should also consider if there's a nuance in the original question. The term \"principaux\" (main) is good, but maybe \"key\" or \"core\" could be more appropriate. Also, making sure the question is clear and asks for the components. Let me check if there's any other way to phrase it. Maybe \"What are the main components of the Transformer model?\" is already good, but perhaps adding \"in detail\" could make it more engaging. However, the user's instruction is to formulate an improved question, so maybe the original is already good, but perhaps the user wants a slightly different version. \n",
      "\n",
      "Alternatively, maybe the user wants to ask about the components in a different context, but based on the given input, the best improvement would be to rephrase the question to be more specific. So, \"What are the main components of the Transformer model?\" seems accurate, but maybe \"What are the primary components of the Transformer model?\" is better. \n",
      "\n",
      "Wait, the original question is in French. The user wants an improved question in French. So, the answer should be in French. Let me confirm. The original question is in French, and the user wants an improved version. So, the improved question should be in French. \n",
      "\n",
      "So, the final answer would be \"Quels sont les principaux composants du modèle Transformer ?\" but that's the same as the original. Wait, no, the user's instruction says to formulate an improved question. Maybe the original is already good, but perhaps the user wants to make it more specific. For example, \"What are the main components of the Transformer model?\" is already good, but maybe \"What are the key components of the Transformer model?\" is better. \n",
      "\n",
      "Alternatively, maybe \"What are the primary building blocks of the Transformer model?\" is more precise. Let me check. The original question is \"principaux composants,\" which is \"main components.\" So, \"principaux composants\" is correct. But maybe \"key components\" is better. \n",
      "\n",
      "I think the best answer is to rephrase the question to be more specific, like \"What are the main components of the Transformer model?\" or \"What are the primary components of the Transformer model?\" ensuring it's in French and clear.\n",
      "</think>\n",
      "\n",
      "**Improved Question:**  \n",
      "Quels sont les principaux composants du modèle Transformer ?\n"
     ]
    }
   ],
   "source": [
    "# Exemple de cas avec un score \"no\"\n",
    "input_no = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=SAMPLE_QUERY),\n",
    "        ToolMessage(content=\"\", tool_call_id=\"1\", name=\"retrieve_blog_posts\", additional_kwargs={\"args\": {\"query\": \"types of reward hacking\"}}),\n",
    "        ToolMessage(content=\"blablabla\", tool_call_id=\"1\")\n",
    "    ]\n",
    "}\n",
    "response = rewrite_question(input_no)\n",
    "print(response[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b3d95c",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Generate an answer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "32c6fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_PROMPT = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, just say that you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "    \"Question: {question} \\n\"\n",
    "    \"Context: {context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d5aae1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(state: MessagesState):\n",
    "    \"\"\"Génère une réponse.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    prompt = GENERATE_PROMPT.format(question=question, context=context)\n",
    "    response = response_model.invoke([HumanMessage(content=prompt)])\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ac236b",
   "metadata": {},
   "source": [
    "#### <a id='toc6_1_1_1_'></a>[Test de la generation de reponse](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cd5af63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, the user is asking about the main components of the Transformer model. The context provided is \"blablabla,\" which seems to be a placeholder or a random string. Since there's no actual information given about the Transformer components in the context, I can't answer based on that. I need to inform the user that I don't have the necessary information to provide the answer. But since the user wants three sentences, I'll make sure to mention that the context is empty and that I don't know the answer.\n",
      "</think>\n",
      "\n",
      "The context provided does not include information about the components of the Transformer model. I don't know the answer.\n"
     ]
    }
   ],
   "source": [
    "# Avec une reponse incomplete du tool\n",
    "input_no = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=SAMPLE_QUERY),\n",
    "        ToolMessage(content=\"\", tool_call_id=\"1\", name=\"retrieve_blog_posts\", additional_kwargs={\"args\": {\"query\": \"types of reward hacking\"}}),\n",
    "        ToolMessage(content=\"blablabla\", tool_call_id=\"1\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = generate_answer(input_no)\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4ced602e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, the user is asking about the main components of the Transformer model. The context provided says the main components are the encoder, decoder, and attention mechanism. I need to make sure I use three sentences and keep it concise. Let me check the context again to confirm. Yep, it lists those three components. I should present that clearly.\n",
      "</think>\n",
      "\n",
      "Les principaux composants du modèle Transformer sont l'encodeur, le décodeur et le mécanisme d'attention.\n"
     ]
    }
   ],
   "source": [
    "# Avec une reponse complete du tool\n",
    "input_yes = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=SAMPLE_QUERY),\n",
    "        ToolMessage(content=\"\", tool_call_id=\"1\", name=\"retrieve_blog_posts\"),\n",
    "        ToolMessage(content=SAMPLE_YES,\n",
    "                   tool_call_id=\"1\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = generate_answer(input_yes)\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d4894c",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[Assemble the graph](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e7897219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# Initialiser le graphe\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Ajouter les nœuds\n",
    "workflow.add_node(\"generate_query_or_response\", generate_query_or_response)\n",
    "workflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\n",
    "workflow.add_node(\"rewrite_question\", rewrite_question)\n",
    "workflow.add_node(\"generate_answer\", generate_answer)\n",
    "\n",
    "# Ajouter les arêtes\n",
    "workflow.add_edge(START, \"generate_query_or_response\")\n",
    "\n",
    "# Ajouter les arêtes conditionnelles\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_query_or_response\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    grade_documents,\n",
    "    {\n",
    "        \"generate_answer\": \"generate_answer\",\n",
    "        \"rewrite_question\": \"rewrite_question\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "workflow.add_edge(\"rewrite_question\", \"generate_query_or_response\")\n",
    "\n",
    "# Compiler le graphe\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac55f38",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[Run the agentic RAG](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8b9558b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Qu'est-ce que le mécanisme d'attention dans le contexte du papier Attention Is All You Need ?\n",
      "Message: <think>\n",
      "Okay, the user is asking about the attention mechanism in the context of the paper \"Attention Is All You Need.\" Let me start by recalling what I know. The paper is about attention mechanisms, specifically how to focus on certain parts of a document. The user mentioned isolated attentions from the word 'its' for positions 5 and 6, which suggests that the attention model is designed to highlight specific words.\n",
      "\n",
      "The user also provided some references to other papers, like [25] Mitchell et al., which talk about building a large corpus. The references seem to be about different aspects of attention mechanisms, such as how to handle long sequences and the use of self-attention. \n",
      "\n",
      "I need to make sure I explain the attention mechanism clearly. The key points from the user's question are that the attention is focused on the word 'its' and that the model uses a neighborhood around the output position. The references mention that the model restricts self-attention to a neighborhood, which increases the path length. \n",
      "\n",
      "I should structure the answer to first explain the attention mechanism, then mention the specific application in the paper, and connect it to the references provided. Also, note that the references are about different models and their applications, which might help the user understand the broader context. \n",
      "\n",
      "Wait, the user's question is in French, but the references are in English. I should check if there's any translation issues, but since the user provided the French text, I'll proceed with the information given. \n",
      "\n",
      "I need to present the answer in a clear, concise manner, ensuring that the attention mechanism's role in the paper is explained, and the references are mentioned to support the explanation. Also, highlight that the model's approach is to focus on specific words and use a neighborhood for self-attention.\n",
      "</think>\n",
      "\n",
      "Le mécanisme d'attention dans le contexte de la paper \"Attention Is All You Need\" se concentre sur la sélection d'éléments spécifiques dans un document, en particulier en concentrant sur la parole \"its\" pour les positions 5 et 6. Ce mécanisme permet de prioriser les informations pertinentes, en utilisant une zone circulaire (ou un cercle) autour de l'élément de sortie pour les modèles de self-attention. Cela améliore la performance des modèles pour les séquences longues en limitant le nombre de connexions possibles, en réduisant ainsi le temps de calcul. Les références fournies illustrent comment ces approches sont appliquées dans d'autres travaux, tels que la construction d'un corpus grand et l'utilisation de modèles de self-attention pour les langues.\n"
     ]
    }
   ],
   "source": [
    "# Définir l'entrée initiale\n",
    "initial_input = MessagesState(messages=[HumanMessage(content= \"Qu'est-ce que le mécanisme d'attention dans le contexte du papier Attention Is All You Need ?\" )])\n",
    "\n",
    "# Exécuter le graphe avec l'entrée initiale\n",
    "try:\n",
    "    result = graph.invoke(initial_input)\n",
    "    # print(\"Final response:\", result)\n",
    "    # Afficher tous les messages intermédiaires\n",
    "    for message in result['messages']:\n",
    "        print(\"Message:\", message.content)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  +-----------+                         \n",
      "                                  | __start__ |                         \n",
      "                                  +-----------+                         \n",
      "                                        *                               \n",
      "                                        *                               \n",
      "                                        *                               \n",
      "                         +----------------------------+                 \n",
      "                         | generate_query_or_response |                 \n",
      "                         +----------------------------+                 \n",
      "                       .....            *            .....              \n",
      "                  .....                 *                 .....         \n",
      "               ...                      *                      .....    \n",
      "    +----------+                        *                           ... \n",
      "    | retrieve |..                      *                             . \n",
      "    +----------+  .....                 *                             . \n",
      "          .            .....            *                             . \n",
      "          .                 .....       *                             . \n",
      "          .                      ...    *                             . \n",
      "+-----------------+           +------------------+                  ... \n",
      "| generate_answer |           | rewrite_question |             .....    \n",
      "+-----------------+****       +------------------+        .....         \n",
      "                       *****                         .....              \n",
      "                            *****               .....                   \n",
      "                                 ***         ...                        \n",
      "                                  +---------+                           \n",
      "                                  | __end__ |                           \n",
      "                                  +---------+                           \n"
     ]
    }
   ],
   "source": [
    "graph.get_graph().print_ascii()\n",
    "# or\n",
    "# print(graph.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e47f16",
   "metadata": {},
   "source": [
    "![alt text](final_graph.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
