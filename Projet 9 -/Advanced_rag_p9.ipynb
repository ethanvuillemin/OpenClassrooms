{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c19a3000",
   "metadata": {},
   "source": [
    "# Rag Avanc√©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadb64dd",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109df51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "class AdvancedRAGState(TypedDict):\n",
    "    \"\"\"√âtat avec fonctionnalit√©s avanc√©es\"\"\"\n",
    "    original_question: str\n",
    "    rewritten_queries: List[str]\n",
    "    query_type: str  # \"simple\", \"multi_doc\", \"comparative\"\n",
    "    documents: List[Document]\n",
    "    retrieved_docs_semantic: List[Document]\n",
    "    retrieved_docs_lexical: List[Document]\n",
    "    merged_docs: List[Document]\n",
    "    reranked_docs: List[Document]\n",
    "    answer: str\n",
    "    confidence_score: float\n",
    "    validation_result: dict\n",
    "    cache_hit: bool\n",
    "    error: str | None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab528e",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820aad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# LLM principal et de validation\n",
    "llm_main = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "llm_validator = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Cache simple (en production: Redis)\n",
    "query_cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1874060",
   "metadata": {},
   "source": [
    "# Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5553d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query(state: AdvancedRAGState) -> AdvancedRAGState:\n",
    "    \"\"\"Agent d'analyse de la requ√™te\"\"\"\n",
    "    print(\"Analyse de la requ√™te...\")\n",
    "    \n",
    "    query = state[\"original_question\"]\n",
    "    \n",
    "    # Prompt pour classifier la requ√™te\n",
    "    classifier_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Classifie la requ√™te en: 'simple' (une info), 'multi_doc' (plusieurs sources), ou 'comparative' (comparaison)\"),\n",
    "        (\"user\", \"Question: {question}\\n\\nR√©ponds uniquement par: simple, multi_doc ou comparative\")\n",
    "    ])\n",
    "    \n",
    "    chain = classifier_prompt | llm_main\n",
    "    result = chain.invoke({\"question\": query})\n",
    "    query_type = result.content.strip().lower()\n",
    "    \n",
    "    state[\"query_type\"] = query_type if query_type in [\"simple\", \"multi_doc\", \"comparative\"] else \"simple\"\n",
    "    print(f\"Type d√©tect√©: {state['query_type']}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def check_cache(state: AdvancedRAGState) -> AdvancedRAGState:\n",
    "    \"\"\"V√©rifie si la r√©ponse est en cache\"\"\"\n",
    "    print(\"V√©rification du cache...\")\n",
    "    \n",
    "    query_key = state[\"original_question\"].lower().strip()\n",
    "    \n",
    "    if query_key in query_cache:\n",
    "        print(\"Cache HIT!\")\n",
    "        cached_result = query_cache[query_key]\n",
    "        state[\"answer\"] = cached_result[\"answer\"]\n",
    "        state[\"confidence_score\"] = cached_result[\"confidence\"]\n",
    "        state[\"cache_hit\"] = True\n",
    "    else:\n",
    "        print(\"Cache MISS\")\n",
    "        state[\"cache_hit\"] = False\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def should_use_cache(state: AdvancedRAGState) -> Literal[\"cached\", \"process\"]:\n",
    "    \"\"\"D√©cision: utiliser le cache ou continuer le traitement\"\"\"\n",
    "    return \"cached\" if state.get(\"cache_hit\") else \"process\"\n",
    "\n",
    "\n",
    "def rewrite_query(state: AdvancedRAGState) -> AdvancedRAGState:\n",
    "    \"\"\"Query rewriting: g√©n√®re plusieurs variantes de la question\"\"\"\n",
    "    print(\"R√©√©criture de la requ√™te...\")\n",
    "    \n",
    "    query = state[\"original_question\"]\n",
    "    \n",
    "    rewriter_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Tu es un expert en reformulation de questions pour am√©liorer la recherche.\n",
    "G√©n√®re 2-3 variantes de la question qui utilisent:\n",
    "- Des synonymes\n",
    "- Des formulations diff√©rentes\n",
    "- Des termes techniques si pertinent\n",
    "\n",
    "Format: une variante par ligne, sans num√©rotation.\"\"\"),\n",
    "        (\"user\", \"Question originale: {question}\")\n",
    "    ])\n",
    "    \n",
    "    chain = rewriter_prompt | llm_main\n",
    "    result = chain.invoke({\"question\": query})\n",
    "    \n",
    "    variants = [line.strip() for line in result.content.split(\"\\n\") if line.strip()]\n",
    "    state[\"rewritten_queries\"] = [query] + variants[:2]\n",
    "    \n",
    "    print(f\"{len(state['rewritten_queries'])} variantes cr√©√©es:\")\n",
    "    for i, q in enumerate(state[\"rewritten_queries\"], 1):\n",
    "        print(f\"   {i}. {q}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def semantic_search(state: AdvancedRAGState) -> AdvancedRAGState:\n",
    "    \"\"\"Recherche s√©mantique avec embeddings\"\"\"\n",
    "    print(\"Recherche s√©mantique...\")\n",
    "    \n",
    "    vectorstore = state.get(\"vectorstore\")\n",
    "    if not vectorstore:\n",
    "        state[\"retrieved_docs_semantic\"] = []\n",
    "        return state\n",
    "    \n",
    "    all_docs = []\n",
    "    seen_content = set()\n",
    "    \n",
    "    # Recherche pour chaque variante de la question\n",
    "    for query in state[\"rewritten_queries\"]:\n",
    "        docs = vectorstore.similarity_search_with_score(query, k=3)\n",
    "        for doc, score in docs:\n",
    "            content_hash = hash(doc.page_content)\n",
    "            if content_hash not in seen_content:\n",
    "                doc.metadata[\"similarity_score\"] = score\n",
    "                all_docs.append(doc)\n",
    "                seen_content.add(content_hash)\n",
    "    \n",
    "    state[\"retrieved_docs_semantic\"] = all_docs[:5]\n",
    "    print(f\"{len(state['retrieved_docs_semantic'])} documents s√©mantiques\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def lexical_search(state: AdvancedRAGState) -> AdvancedRAGState:\n",
    "    \"\"\"Recherche lexicale BM25\"\"\"\n",
    "    print(\"Recherche lexicale (BM25)...\")\n",
    "    \n",
    "    documents = state.get(\"documents\", [])\n",
    "    if not documents:\n",
    "        state[\"retrieved_docs_lexical\"] = []\n",
    "        return state\n",
    "    \n",
    "    # Cr√©ation du retriever BM25\n",
    "    bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "    bm25_retriever.k = 3\n",
    "    \n",
    "    # Recherche pour la question originale\n",
    "    docs = bm25_retriever.get_relevant_documents(state[\"original_question\"])\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc.metadata[\"search_type\"] = \"lexical\"\n",
    "    \n",
    "    state[\"retrieved_docs_lexical\"] = docs\n",
    "    print(f\"{len(docs)} documents lexicaux\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def hybrid_merge(state: AdvancedRAGState) -> AdvancedRAGState:\n",
    "    \"\"\"Fusionne et d√©duplique les r√©sultats s√©mantiques et lexicaux\"\"\"\n",
    "    print(\"Fusion hybride des r√©sultats...\")\n",
    "    \n",
    "    semantic_docs = state.get(\"retrieved_docs_semantic\", [])\n",
    "    lexical_docs = state.get(\"retrieved_docs_lexical\", [])\n",
    "    \n",
    "    # Fusion avec d√©duplication\n",
    "    merged = {}\n",
    "    \n",
    "    # Pond√©ration: s√©mantique = 0.7, lexical = 0.3\n",
    "    for doc in semantic_docs:\n",
    "        content_hash = hash(doc.page_content)\n",
    "        merged[content_hash] = {\n",
    "            \"doc\": doc,\n",
    "            \"score\": 0.7 * (1 - doc.metadata.get(\"similarity_score\", 0))\n",
    "        }\n",
    "    \n",
    "    for doc in lexical_docs:\n",
    "        content_hash = hash(doc.page_content)\n",
    "        if content_hash in merged:\n",
    "            merged[content_hash][\"score\"] += 0.3\n",
    "        else:\n",
    "            merged[content_hash] = {\"doc\": doc, \"score\": 0.3}\n",
    "    \n",
    "    # Tri par score d√©croissant\n",
    "    sorted_docs = sorted(\n",
    "        merged.values(),\n",
    "        key=lambda x: x[\"score\"],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    state[\"merged_docs\"] = [item[\"doc\"] for item in sorted_docs[:5]]\n",
    "    print(f\"{len(state['merged_docs'])} documents fusionn√©s\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def rerank_documents(state: AdvancedRAGState) -> AdvancedRAGState:\n",
    "    \"\"\"Reranking avec un LLM pour affiner la pertinence\"\"\"\n",
    "    print(\"üéØ Reranking des documents...\")\n",
    "    \n",
    "    docs = state.get(\"merged_docs\", [])\n",
    "    if not docs:\n",
    "        state[\"reranked_docs\"] = []\n",
    "        return state\n",
    "    \n",
    "    # Pour une vraie impl√©mentation, utiliser un cross-encoder\n",
    "    # Ici, simulation simple avec LLM\n",
    "    \n",
    "    rerank_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"√âvalue la pertinence de chaque document pour r√©pondre √† la question.\n",
    "Score de 0 √† 10. Format: Document X: score\"\"\"),\n",
    "        (\"user\", \"\"\"Question: {question}\n",
    "\n",
    "Documents:\n",
    "{documents}\"\"\")\n",
    "    ])\n",
    "    \n",
    "    docs_text = \"\\n\\n\".join([\n",
    "        f\"Document {i+1}:\\n{doc.page_content[:300]}...\"\n",
    "        for i, doc in enumerate(docs)\n",
    "    ])\n",
    "    \n",
    "    chain = rerank_prompt | llm_validator\n",
    "    result = chain.invoke({\n",
    "        \"question\": state[\"original_question\"],\n",
    "        \"documents\": docs_text\n",
    "    })\n",
    "    \n",
    "    # Parsing simple des scores (en production: plus robuste)\n",
    "    scores = [7, 6, 5, 4, 3]  # Fallback par d√©faut\n",
    "    \n",
    "    try:\n",
    "        lines = result.content.split(\"\\n\")\n",
    "        scores = []\n",
    "        for line in lines:\n",
    "            if \":\" in line and any(str(i) in line for i in range(11)):\n",
    "                score_part = line.split(\":\")[-1].strip()\n",
    "                score = float(''.join(c for c in score_part if c.isdigit() or c == '.'))\n",
    "                scores.append(score)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Associer scores et trier\n",
    "    docs_with_scores = list(zip(docs, scores[:len(docs)]))\n",
    "    docs_with_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    state[\"reranked_docs\"] = [doc for doc, _ in docs_with_scores[:3]]\n",
    "    print(f\"Top {len(state['reranked_docs'])} documents apr√®s reranking\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def generate_answer_advanced(state: AdvancedRAGState) -> AdvancedRAGState:\n",
    "    \"\"\"G√©n√©ration avanc√©e avec m√©tadonn√©es\"\"\"\n",
    "    print(\"G√©n√©ration de la r√©ponse avanc√©e...\")\n",
    "    \n",
    "    docs = state.get(\"reranked_docs\", [])\n",
    "    if not docs:\n",
    "        state[\"answer\"] = \"Je n'ai pas trouv√© suffisamment d'informations pertinentes.\"\n",
    "        state[\"confidence_score\"] = 0.0\n",
    "        return state\n",
    "    \n",
    "    # Contexte enrichi avec m√©tadonn√©es\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source = doc.metadata.get(\"source\", \"inconnu\")\n",
    "        context_parts.append(f\"[Source: {source}]\\n{doc.page_content}\")\n",
    "    \n",
    "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Prompt avanc√© avec instructions de citation\n",
    "    advanced_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Tu es un assistant expert qui r√©pond avec pr√©cision en citant ses sources.\n",
    "\n",
    "Instructions:\n",
    "- Utilise UNIQUEMENT les informations des documents fournis\n",
    "- Cite la source entre crochets [Source: X] apr√®s chaque affirmation\n",
    "- Si plusieurs sources, compare et synth√©tise\n",
    "- Si information manquante, dis-le clairement\n",
    "- Sois concis mais complet\"\"\"),\n",
    "        (\"user\", \"\"\"Documents:\\n{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "R√©ponds de mani√®re structur√©e et cite tes sources.\"\"\")\n",
    "    ])\n",
    "    \n",
    "    chain = advanced_prompt | llm_main\n",
    "    response = chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"question\": state[\"original_question\"]\n",
    "    })\n",
    "    \n",
    "    state[\"answer\"] = response.content\n",
    "    state[\"confidence_score\"] = min(len(docs) / 3.0, 1.0)  # Score bas√© sur nb de docs\n",
    "    print(f\" R√©ponse g√©n√©r√©e (confiance: {state['confidence_score']:.2f})\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def validate_answer(state: AdvancedRAGState) -> AdvancedRAGState:\n",
    "    \"\"\"Valide la r√©ponse pour d√©tecter hallucinations\"\"\"\n",
    "    print(\"Validation de la r√©ponse...\")\n",
    "    \n",
    "    answer = state.get(\"answer\", \"\")\n",
    "    docs = state.get(\"reranked_docs\", [])\n",
    "    \n",
    "    if not answer or not docs:\n",
    "        state[\"validation_result\"] = {\"valid\": False, \"reason\": \"Donn√©es insuffisantes\"}\n",
    "        return state\n",
    "    \n",
    "    # Validation avec LLM\n",
    "    validation_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"V√©rifie si la r√©ponse est fid√®le aux documents sources.\n",
    "D√©tecte:\n",
    "- Hallucinations (infos non pr√©sentes)\n",
    "- Contradictions\n",
    "- Extrapolations excessives\n",
    "\n",
    "Format JSON: {\"valid\": true/false, \"issues\": [\"liste\"], \"confidence\": 0-100}\"\"\"),\n",
    "        (\"user\", \"\"\"Documents sources:\n",
    "{context}\n",
    "\n",
    "R√©ponse √† v√©rifier:\n",
    "{answer}\n",
    "\n",
    "Validation:\"\"\")\n",
    "    ])\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    chain = validation_prompt | llm_validator\n",
    "    result = chain.invoke({\"context\": context, \"answer\": answer})\n",
    "    \n",
    "    # Parsing simple (en production: JSON parser robuste)\n",
    "    validation = {\n",
    "        \"valid\": \"true\" in result.content.lower() or \"valid\" in result.content.lower(),\n",
    "        \"confidence\": 85,  # Valeur par d√©faut\n",
    "        \"issues\": []\n",
    "    }\n",
    "    \n",
    "    state[\"validation_result\"] = validation\n",
    "    print(f\"Validation: {'‚úì Valide' if validation['valid'] else '‚úó Probl√®me d√©tect√©'}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def save_to_cache(state: AdvancedRAGState) -> AdvancedRAGState:\n",
    "    \"\"\"Sauvegarde dans le cache si validation OK\"\"\"\n",
    "    print(\"Sauvegarde en cache...\")\n",
    "    \n",
    "    if state.get(\"validation_result\", {}).get(\"valid\", False):\n",
    "        query_key = state[\"original_question\"].lower().strip()\n",
    "        query_cache[query_key] = {\n",
    "            \"answer\": state[\"answer\"],\n",
    "            \"confidence\": state[\"confidence_score\"]\n",
    "        }\n",
    "        print(\"R√©ponse mise en cache\")\n",
    "    else:\n",
    "        print(\"R√©ponse non cach√©e (validation √©chou√©e)\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def route_by_complexity(state: AdvancedRAGState) -> Literal[\"simple_path\", \"complex_path\"]:\n",
    "    \"\"\"Routing intelligent selon la complexit√©\"\"\"\n",
    "    query_type = state.get(\"query_type\", \"simple\")\n",
    "    \n",
    "    if query_type in [\"multi_doc\", \"comparative\"]:\n",
    "        print(\"‚Üí Route complexe\")\n",
    "        return \"complex_path\"\n",
    "    else:\n",
    "        print(\"‚Üí Route simple\")\n",
    "        return \"simple_path\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e8d982",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_rag_graph():\n",
    "    \"\"\"Cr√©e le graphe RAG avanc√© avec agents et routing\"\"\"\n",
    "    \n",
    "    workflow = StateGraph(AdvancedRAGState)\n",
    "    \n",
    "    # Phase 1: Analyse et cache\n",
    "    workflow.add_node(\"analyze\", analyze_query)\n",
    "    workflow.add_node(\"cache_check\", check_cache)\n",
    "    workflow.add_node(\"cached_response\", lambda s: s)  # N≈ìud passthrough\n",
    "    \n",
    "    # Phase 2: Query processing\n",
    "    workflow.add_node(\"rewrite\", rewrite_query)\n",
    "    \n",
    "    # Phase 3: Recherche\n",
    "    workflow.add_node(\"semantic\", semantic_search)\n",
    "    workflow.add_node(\"lexical\", lexical_search)\n",
    "    workflow.add_node(\"merge\", hybrid_merge)\n",
    "    workflow.add_node(\"rerank\", rerank_documents)\n",
    "    \n",
    "    # Phase 4: G√©n√©ration et validation\n",
    "    workflow.add_node(\"generate\", generate_answer_advanced)\n",
    "    workflow.add_node(\"validate\", validate_answer)\n",
    "    workflow.add_node(\"cache_save\", save_to_cache)\n",
    "    \n",
    "    # Flux du graphe\n",
    "    workflow.set_entry_point(\"analyze\")\n",
    "    workflow.add_edge(\"analyze\", \"cache_check\")\n",
    "    \n",
    "    # Branchement conditionnel: cache ou traitement\n",
    "    workflow.add_conditional_edges(\n",
    "        \"cache_check\",\n",
    "        should_use_cache,\n",
    "        {\n",
    "            \"cached\": \"cached_response\",\n",
    "            \"process\": \"rewrite\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    workflow.add_edge(\"cached_response\", END)\n",
    "    \n",
    "    # Branchement selon complexit√©\n",
    "    workflow.add_conditional_edges(\n",
    "        \"rewrite\",\n",
    "        route_by_complexity,\n",
    "        {\n",
    "            \"simple_path\": \"semantic\",\n",
    "            \"complex_path\": \"semantic\"  # Les deux passent par semantic d'abord\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    workflow.add_edge(\"semantic\", \"lexical\")\n",
    "    workflow.add_edge(\"lexical\", \"merge\")\n",
    "    workflow.add_edge(\"merge\", \"rerank\")\n",
    "    workflow.add_edge(\"rerank\", \"generate\")\n",
    "    workflow.add_edge(\"generate\", \"validate\")\n",
    "    workflow.add_edge(\"validate\", \"cache_save\")\n",
    "    workflow.add_edge(\"cache_save\", END)\n",
    "    \n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd276ab1",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb84228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "    \n",
    "    # Documents d'exemple\n",
    "    sample_docs = [\n",
    "        Document(\n",
    "            page_content=\"\"\"Notre politique de remboursement:\n",
    "- D√©lai de r√©tractation: 30 jours calendaires\n",
    "- Remboursement int√©gral si produit non ouvert et dans emballage d'origine\n",
    "- Frais de retour √† la charge du client sauf produit d√©fectueux\n",
    "- Remboursement sous 10 jours apr√®s r√©ception du retour\"\"\",\n",
    "            metadata={\"source\": \"politique_remboursement.pdf\"}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=\"\"\"Garanties par forfait:\n",
    "Pack Basic: \n",
    "- 1 an garantie constructeur\n",
    "- Support par email sous 48h\n",
    "\n",
    "Pack Premium:\n",
    "- 3 ans garantie constructeur + 2 ans extension\n",
    "- Support prioritaire 24/7 par t√©l√©phone\n",
    "- Pr√™t de mat√©riel en cas de panne\n",
    "\n",
    "Pack Ultimate:\n",
    "- 5 ans garantie tous risques (casse, vol, oxydation)\n",
    "- Remplacement sous 24h garanti\n",
    "- Support d√©di√© avec interlocuteur unique\n",
    "- Maintenance pr√©ventive annuelle gratuite\"\"\",\n",
    "            metadata={\"source\": \"garanties_forfaits.pdf\"}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=\"\"\"D√©lais et modes de livraison:\n",
    "Standard (gratuit):\n",
    "- France m√©tropolitaine: 3-5 jours ouvr√©s\n",
    "- Corse: 5-7 jours\n",
    "- DOM-TOM: 10-15 jours\n",
    "\n",
    "Express (19.90‚Ç¨):\n",
    "- France: 24-48h\n",
    "- Non disponible DOM-TOM\n",
    "\n",
    "Premium (39.90‚Ç¨):\n",
    "- Livraison le lendemain avant 12h\n",
    "- France m√©tropolitaine uniquement\n",
    "- Commande avant 15h\"\"\",\n",
    "            metadata={\"source\": \"livraison_delais.pdf\"}\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Preprocessing des documents\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    chunks = splitter.split_documents(sample_docs)\n",
    "    \n",
    "    # Cr√©ation vectorstore\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "    \n",
    "    # Cr√©ation du graphe\n",
    "    app = create_advanced_rag_graph()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üöÄ RAG AVANC√â - D√âMONSTRATION\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Test 1: Question simple\n",
    "    state1 = {\n",
    "        \"original_question\": \"Quel est le d√©lai de remboursement ?\",\n",
    "        \"rewritten_queries\": [],\n",
    "        \"query_type\": \"\",\n",
    "        \"documents\": chunks,\n",
    "        \"retrieved_docs_semantic\": [],\n",
    "        \"retrieved_docs_lexical\": [],\n",
    "        \"merged_docs\": [],\n",
    "        \"reranked_docs\": [],\n",
    "        \"answer\": \"\",\n",
    "        \"confidence_score\": 0.0,\n",
    "        \"validation_result\": {},\n",
    "        \"cache_hit\": False,\n",
    "        \"error\": None,\n",
    "        \"vectorstore\": vectorstore\n",
    "    }\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
