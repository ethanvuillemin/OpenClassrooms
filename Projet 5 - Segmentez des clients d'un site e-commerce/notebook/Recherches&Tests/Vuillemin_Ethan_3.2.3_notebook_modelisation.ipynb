{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook de modelisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation des dependances avec uv\n",
    "# !uv pip install scikit-learn numpy hdbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports des libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import cluster, metrics, mixture\n",
    "from sklearn.datasets import make_blobs\n",
    "import time\n",
    "import hdbscan  # Pour HDBSCAN\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction d'evaluation d'un modèle de clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_algorithm(name, algorithm, data):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Fit et prédire les labels\n",
    "    labels = algorithm.fit_predict(data)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Calcul des scores\n",
    "    silhouette_score = metrics.silhouette_score(data, labels) if len(set(labels)) > 1 else None\n",
    "    davies_bouldin_score = metrics.davies_bouldin_score(data, labels) if len(set(labels)) > 1 else None\n",
    "    calinski_harabasz_score = metrics.calinski_harabasz_score(data, labels) if len(set(labels)) > 1 else None\n",
    "\n",
    "    return {\n",
    "        \"Algorithm\": name,\n",
    "        \"Time (s)\": elapsed_time,\n",
    "        \"Silhouette Score\": silhouette_score,\n",
    "        \"Davies-Bouldin Score\": davies_bouldin_score,\n",
    "        \"Calinski-Harabasz Score\": calinski_harabasz_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark des models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les modèles de clustering dispo avec [sklearn](https://scikit-learn.org/stable/modules/clustering.html) et son tableau comparatif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liste des algorithmes à comparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_cluster_value = 3\n",
    "min_sample_value = 1000\n",
    "\n",
    "model_list = {\n",
    "    'kmeans': cluster.KMeans(n_clusters=n_cluster_value),\n",
    "    'minibatchkmeans': cluster.MiniBatchKMeans(n_clusters=n_cluster_value),\n",
    "    'hdbscan': hdbscan.HDBSCAN(min_cluster_size=min_sample_value),\n",
    "    'gmm': mixture.GaussianMixture(n_components=n_cluster_value),\n",
    "    'birch': cluster.Birch(n_clusters=n_cluster_value),\n",
    "    'agglomerative': cluster.AgglomerativeClustering(n_clusters=n_cluster_value),\n",
    "    'dbscan': cluster.DBSCAN(eps=0.5, min_samples=min_sample_value),\n",
    "    'optics': cluster.OPTICS(min_samples=min_sample_value),\n",
    "    'meanshift': cluster.MeanShift(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark RFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import des données\n",
    "rfm = pd.read_excel('RFM.xlsx')\n",
    "rfm = rfm.drop(['customer_unique_id', 'Segment'], axis=1).dropna()\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On prend 10% des données pour gagner du temps d'execution\n",
    "rfm_sample = rfm.sample(frac=0.1, random_state=42)\n",
    "print(rfm_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement des modèle (Utilisation du multithreading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Exécution en parallèle avec joblib\n",
    "results = Parallel(n_jobs=-1)(delayed(evaluate_algorithm)(name, algorithm, rfm_sample)for name, algorithm in model_list.items())\n",
    "\n",
    "# Création d'un DataFrame pour afficher les résultats\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score : Plus il est élevé, mieux c'est\n",
    "# Davies-Bouldin Score : Plus il est faible, mieux c'est \n",
    "# Calinski-Harabasz Score : Plus il est élevé, mieux c'est\n",
    "\n",
    "results_df.sort_values(by=['Silhouette Score', 'Davies-Bouldin Score', 'Calinski-Harabasz Score'], ascending=[False, True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the best number of cluster (Plot metrics per number of cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import numpy as np\n",
    "\n",
    "def plot_metrics_for_each_model(data, max_k=10):\n",
    "    metrics = {\n",
    "        'kmeans': {'inertia': [], 'silhouette': [], 'davies_bouldin': [], 'calinski_harabasz': []},\n",
    "        'birch': {'inertia': [], 'silhouette': [], 'davies_bouldin': [], 'calinski_harabasz': []},\n",
    "        'agglomerative': {'silhouette': [], 'davies_bouldin': [], 'calinski_harabasz': []},\n",
    "    }\n",
    "\n",
    "    k_values = range(2, max_k + 1)\n",
    "\n",
    "    for k in k_values:\n",
    "        # KMeans\n",
    "        kmeans = cluster.KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(data)\n",
    "        metrics['kmeans']['inertia'].append(kmeans.inertia_)\n",
    "        if len(set(labels)) > 1:\n",
    "            metrics['kmeans']['silhouette'].append(silhouette_score(data, labels))\n",
    "            metrics['kmeans']['davies_bouldin'].append(davies_bouldin_score(data, labels))\n",
    "            metrics['kmeans']['calinski_harabasz'].append(calinski_harabasz_score(data, labels))\n",
    "\n",
    "        # Birch\n",
    "        birch = cluster.Birch(n_clusters=k)\n",
    "        labels = birch.fit_predict(data)\n",
    "        if len(set(labels)) > 1:\n",
    "            metrics['birch']['inertia'].append(birch.subcluster_centers_.shape[0])  # Approximation of inertia\n",
    "            metrics['birch']['silhouette'].append(silhouette_score(data, labels))\n",
    "            metrics['birch']['davies_bouldin'].append(davies_bouldin_score(data, labels))\n",
    "            metrics['birch']['calinski_harabasz'].append(calinski_harabasz_score(data, labels))\n",
    "\n",
    "        # Agglomerative Clustering\n",
    "        agg = cluster.AgglomerativeClustering(n_clusters=k)\n",
    "        labels = agg.fit_predict(data)\n",
    "        if len(set(labels)) > 1:\n",
    "            metrics['agglomerative']['silhouette'].append(silhouette_score(data, labels))\n",
    "            metrics['agglomerative']['davies_bouldin'].append(davies_bouldin_score(data, labels))\n",
    "            metrics['agglomerative']['calinski_harabasz'].append(calinski_harabasz_score(data, labels))\n",
    "\n",
    "    # Tracé des métriques pour chaque modèle\n",
    "    models = ['kmeans', 'birch', 'agglomerative']\n",
    "    for model in models:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "\n",
    "        # Elbow Method\n",
    "        plt.subplot(2, 2, 1)\n",
    "        if 'inertia' in metrics[model] and len(metrics[model]['inertia']) > 0:\n",
    "            plt.plot(k_values, metrics[model]['inertia'], 'bo-', markersize=4)\n",
    "            plt.xlabel('Number of clusters, k')\n",
    "            plt.ylabel('Inertia')\n",
    "            plt.title(f'{model.capitalize()} - Elbow Method')\n",
    "            plt.grid(True)\n",
    "        else:\n",
    "            plt.title(f'{model.capitalize()} - Elbow Method (No Data)')\n",
    "\n",
    "        # Silhouette Score\n",
    "        plt.subplot(2, 2, 2)\n",
    "        if len(metrics[model]['silhouette']) > 0:\n",
    "            plt.plot(k_values, metrics[model]['silhouette'], 'go-', markersize=4)\n",
    "            plt.xlabel('Number of clusters, k')\n",
    "            plt.ylabel('Silhouette Score')\n",
    "            plt.title(f'{model.capitalize()} - Silhouette Score')\n",
    "            plt.grid(True)\n",
    "        else:\n",
    "            plt.title(f'{model.capitalize()} - Silhouette Score (No Data)')\n",
    "\n",
    "        # Davies-Bouldin Score\n",
    "        plt.subplot(2, 2, 3)\n",
    "        if len(metrics[model]['davies_bouldin']) > 0:\n",
    "            plt.plot(k_values, metrics[model]['davies_bouldin'], 'ro-', markersize=4)\n",
    "            plt.xlabel('Number of clusters, k')\n",
    "            plt.ylabel('Davies-Bouldin Score')\n",
    "            plt.title(f'{model.capitalize()} - Davies-Bouldin Score')\n",
    "            plt.grid(True)\n",
    "        else:\n",
    "            plt.title(f'{model.capitalize()} - Davies-Bouldin Score (No Data)')\n",
    "\n",
    "        # Calinski-Harabasz Score\n",
    "        plt.subplot(2, 2, 4)\n",
    "        if len(metrics[model]['calinski_harabasz']) > 0:\n",
    "            plt.plot(k_values, metrics[model]['calinski_harabasz'], 'mo-', markersize=4)\n",
    "            plt.xlabel('Number of clusters, k')\n",
    "            plt.ylabel('Calinski-Harabasz Score')\n",
    "            plt.title(f'{model.capitalize()} - Calinski-Harabasz Score')\n",
    "            plt.grid(True)\n",
    "        else:\n",
    "            plt.title(f'{model.capitalize()} - Calinski-Harabasz Score (No Data)')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(f'{model.capitalize()} Clustering Metrics', fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "# Exemple d'utilisation avec des données fictives\n",
    "# data = ... (votre jeu de données)\n",
    "plot_metrics_for_each_model(rfm_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correspondance avec les sagments defini dans l'EDA\n",
    "segment_order = ['Champions', 'Loyaux', 'Loyalistes potentiels', 'À réactiver', 'À risque', 'Perdus']\n",
    "\n",
    "3 Correspond au 3 persona les plus presents et 6 correspond a tous les personas !\n",
    "\n",
    "![{A84F2560-6E5B-4FE6-8987-0BEFCC5E62EC}.png](attachment:{A84F2560-6E5B-4FE6-8987-0BEFCC5E62EC}.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation des clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vue 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Supposons que 'rfm_sample' est votre rfm_sampleFrame RFM\n",
    "# rfm_sample = pd.rfm_sampleFrame(...)\n",
    "\n",
    "# Appliquer KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "rfm_sample['Cluster'] = kmeans.fit_predict(rfm_sample[['R', 'F', 'M']])\n",
    "\n",
    "# Analyser les centres des clusters\n",
    "centers = pd.DataFrame(kmeans.cluster_centers_, columns=['R', 'F', 'M'])\n",
    "# print(\"Centres des clusters :\")\n",
    "# print(centers)\n",
    "\n",
    "# Calculer les statistiques descriptives pour chaque cluster\n",
    "cluster_stats = rfm_sample.groupby('Cluster')[['R', 'F', 'M']].agg(['mean', 'median', 'std'])\n",
    "# print(\"Statistiques descriptives des clusters :\")\n",
    "# print(cluster_stats)\n",
    "\n",
    "# Visualiser les clusters\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Graphique Récence vs Fréquence\n",
    "plt.subplot(1, 3, 1)\n",
    "for cluster in range(3):\n",
    "    plt.scatter(rfm_sample[rfm_sample['Cluster'] == cluster]['R'], rfm_sample[rfm_sample['Cluster'] == cluster]['F'], label=f'Cluster {cluster}')\n",
    "plt.xlabel('Récence')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Récence vs Fréquence')\n",
    "plt.legend()\n",
    "\n",
    "# Graphique Fréquence vs Monetary\n",
    "plt.subplot(1, 3, 2)\n",
    "for cluster in range(3):\n",
    "    plt.scatter(rfm_sample[rfm_sample['Cluster'] == cluster]['F'], rfm_sample[rfm_sample['Cluster'] == cluster]['M'], label=f'Cluster {cluster}')\n",
    "plt.xlabel('Fréquence')\n",
    "plt.ylabel('Monetary')\n",
    "plt.title('Fréquence vs Monetary')\n",
    "plt.legend()\n",
    "\n",
    "# Graphique Récence vs Monetary\n",
    "plt.subplot(1, 3, 3)\n",
    "for cluster in range(3):\n",
    "    plt.scatter(rfm_sample[rfm_sample['Cluster'] == cluster]['R'], rfm_sample[rfm_sample['Cluster'] == cluster]['M'], label=f'Cluster {cluster}')\n",
    "plt.xlabel('Récence')\n",
    "plt.ylabel('Monetary')\n",
    "plt.title('Récence vs Monetary')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vue 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Supposons que 'data' est votre DataFrame RFM avec une colonne 'Cluster'\n",
    "# data = pd.DataFrame(...)\n",
    "\n",
    "# Créer un graphique 3D avec Plotly\n",
    "fig = px.scatter_3d(rfm_sample, x='R', y='F', z='M', color='Cluster',\n",
    "                    labels={'R': 'Récence', 'F': 'Fréquence', 'M': 'Monetary', 'Cluster': 'Cluster'},\n",
    "                    title='Clustering RFM en 3D')\n",
    "\n",
    "# Afficher le graphique\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark RFMLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import des données\n",
    "rfmls = pd.read_excel('RFMLS.xlsx')\n",
    "rfmls = rfmls.drop(['customer_unique_id', 'Unnamed: 0'], axis=1).dropna()\n",
    "rfmls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On prend 10% des données pour gagner du temps d'execution\n",
    "rfmls_sample = rfmls.sample(frac=0.1, random_state=42)\n",
    "print(rfmls_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialisation du FeatureHasher\n",
    "hasher = FeatureHasher(n_features=50, input_type='string')\n",
    "\n",
    "# Transformation de la colonne 'L' en une liste de listes\n",
    "hashed_features = hasher.transform([[city] for city in rfmls_sample['L']])\n",
    "\n",
    "# Initialisation du PCA\n",
    "pca = PCA(n_components=1)  # Réduction à une seule dimension\n",
    "\n",
    "# Application du PCA sur les caractéristiques hachées\n",
    "hashed_features_pca = pca.fit_transform(hashed_features.toarray())\n",
    "\n",
    "# Ajout de la composante principale au DataFrame original\n",
    "rfmls_sample['L_pca'] = hashed_features_pca\n",
    "\n",
    "print(rfmls_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement des modèle (Utilisation du multithreading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmls_sample_train = rfmls_sample.drop('L', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Exécution en parallèle avec joblib\n",
    "results = Parallel(n_jobs=-1)(delayed(evaluate_algorithm)(name, algorithm, rfmls_sample_train)for name, algorithm in model_list.items())\n",
    "\n",
    "# Création d'un DataFrame pour afficher les résultats\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score : Plus il est élevé, mieux c'est\n",
    "# Davies-Bouldin Score : Plus il est faible, mieux c'est \n",
    "# Calinski-Harabasz Score : Plus il est élevé, mieux c'est\n",
    "\n",
    "results_df.sort_values(by=['Silhouette Score', 'Davies-Bouldin Score', 'Calinski-Harabasz Score'], ascending=[False, True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the best number of cluster (Plot metrics per number of cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import numpy as np\n",
    "\n",
    "def plot_metrics_for_each_model(data, max_k=10):\n",
    "    metrics = {\n",
    "        'kmeans': {'inertia': [], 'silhouette': [], 'davies_bouldin': [], 'calinski_harabasz': []},\n",
    "        'birch': {'inertia': [], 'silhouette': [], 'davies_bouldin': [], 'calinski_harabasz': []},\n",
    "        'agglomerative': {'silhouette': [], 'davies_bouldin': [], 'calinski_harabasz': []},\n",
    "    }\n",
    "\n",
    "    k_values = range(2, max_k + 1)\n",
    "\n",
    "    for k in k_values:\n",
    "        # KMeans\n",
    "        kmeans = cluster.KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(data)\n",
    "        metrics['kmeans']['inertia'].append(kmeans.inertia_)\n",
    "        if len(set(labels)) > 1:\n",
    "            metrics['kmeans']['silhouette'].append(silhouette_score(data, labels))\n",
    "            metrics['kmeans']['davies_bouldin'].append(davies_bouldin_score(data, labels))\n",
    "            metrics['kmeans']['calinski_harabasz'].append(calinski_harabasz_score(data, labels))\n",
    "\n",
    "        # Birch\n",
    "        birch = cluster.Birch(n_clusters=k)\n",
    "        labels = birch.fit_predict(data)\n",
    "        if len(set(labels)) > 1:\n",
    "            metrics['birch']['inertia'].append(birch.subcluster_centers_.shape[0])  # Approximation of inertia\n",
    "            metrics['birch']['silhouette'].append(silhouette_score(data, labels))\n",
    "            metrics['birch']['davies_bouldin'].append(davies_bouldin_score(data, labels))\n",
    "            metrics['birch']['calinski_harabasz'].append(calinski_harabasz_score(data, labels))\n",
    "\n",
    "        # Agglomerative Clustering\n",
    "        agg = cluster.AgglomerativeClustering(n_clusters=k)\n",
    "        labels = agg.fit_predict(data)\n",
    "        if len(set(labels)) > 1:\n",
    "            metrics['agglomerative']['silhouette'].append(silhouette_score(data, labels))\n",
    "            metrics['agglomerative']['davies_bouldin'].append(davies_bouldin_score(data, labels))\n",
    "            metrics['agglomerative']['calinski_harabasz'].append(calinski_harabasz_score(data, labels))\n",
    "\n",
    "    # Tracé des métriques pour chaque modèle\n",
    "    models = ['kmeans', 'birch', 'agglomerative']\n",
    "    for model in models:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "\n",
    "        # Elbow Method\n",
    "        plt.subplot(2, 2, 1)\n",
    "        if 'inertia' in metrics[model] and len(metrics[model]['inertia']) > 0:\n",
    "            plt.plot(k_values, metrics[model]['inertia'], 'bo-', markersize=4)\n",
    "            plt.xlabel('Number of clusters, k')\n",
    "            plt.ylabel('Inertia')\n",
    "            plt.title(f'{model.capitalize()} - Elbow Method')\n",
    "            plt.grid(True)\n",
    "        else:\n",
    "            plt.title(f'{model.capitalize()} - Elbow Method (No Data)')\n",
    "\n",
    "        # Silhouette Score\n",
    "        plt.subplot(2, 2, 2)\n",
    "        if len(metrics[model]['silhouette']) > 0:\n",
    "            plt.plot(k_values, metrics[model]['silhouette'], 'go-', markersize=4)\n",
    "            plt.xlabel('Number of clusters, k')\n",
    "            plt.ylabel('Silhouette Score')\n",
    "            plt.title(f'{model.capitalize()} - Silhouette Score')\n",
    "            plt.grid(True)\n",
    "        else:\n",
    "            plt.title(f'{model.capitalize()} - Silhouette Score (No Data)')\n",
    "\n",
    "        # Davies-Bouldin Score\n",
    "        plt.subplot(2, 2, 3)\n",
    "        if len(metrics[model]['davies_bouldin']) > 0:\n",
    "            plt.plot(k_values, metrics[model]['davies_bouldin'], 'ro-', markersize=4)\n",
    "            plt.xlabel('Number of clusters, k')\n",
    "            plt.ylabel('Davies-Bouldin Score')\n",
    "            plt.title(f'{model.capitalize()} - Davies-Bouldin Score')\n",
    "            plt.grid(True)\n",
    "        else:\n",
    "            plt.title(f'{model.capitalize()} - Davies-Bouldin Score (No Data)')\n",
    "\n",
    "        # Calinski-Harabasz Score\n",
    "        plt.subplot(2, 2, 4)\n",
    "        if len(metrics[model]['calinski_harabasz']) > 0:\n",
    "            plt.plot(k_values, metrics[model]['calinski_harabasz'], 'mo-', markersize=4)\n",
    "            plt.xlabel('Number of clusters, k')\n",
    "            plt.ylabel('Calinski-Harabasz Score')\n",
    "            plt.title(f'{model.capitalize()} - Calinski-Harabasz Score')\n",
    "            plt.grid(True)\n",
    "        else:\n",
    "            plt.title(f'{model.capitalize()} - Calinski-Harabasz Score (No Data)')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(f'{model.capitalize()} Clustering Metrics', fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "# Exemple d'utilisation avec des données fictives\n",
    "# data = ... (votre jeu de données)\n",
    "plot_metrics_for_each_model(rfmls_sample_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correspondance avec les sagments defini dans l'EDA\n",
    "segment_order = ['Champions', 'Loyaux', 'Loyalistes potentiels', 'À réactiver', 'À risque', 'Perdus']\n",
    "\n",
    "3 Correspond au 3 persona les plus presents et 6 correspond a tous les personas !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation des clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vue 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# Appliquer KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "rfmls_sample['Cluster'] = kmeans.fit_predict(rfmls_sample[['R', 'F', 'M']])\n",
    "\n",
    "# Analyser les centres des clusters\n",
    "centers = pd.DataFrame(kmeans.cluster_centers_, columns=['R', 'F', 'M'])\n",
    "# print(\"Centres des clusters :\")\n",
    "# print(centers)\n",
    "\n",
    "# Calculer les statistiques descriptives pour chaque cluster\n",
    "cluster_stats = rfmls_sample.groupby('Cluster')[['R', 'F', 'M']].agg(['mean', 'median', 'std'])\n",
    "# print(\"Statistiques descriptives des clusters :\")\n",
    "# print(cluster_stats)\n",
    "\n",
    "# Visualiser les clusters\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Graphique Récence vs Fréquence\n",
    "plt.subplot(1, 3, 1)\n",
    "for cluster in range(3):\n",
    "    plt.scatter(rfmls_sample[rfmls_sample['Cluster'] == cluster]['R'], rfmls_sample[rfmls_sample['Cluster'] == cluster]['F'], label=f'Cluster {cluster}')\n",
    "plt.xlabel('Récence')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Récence vs Fréquence')\n",
    "plt.legend()\n",
    "\n",
    "# Graphique Fréquence vs Monetary\n",
    "plt.subplot(1, 3, 2)\n",
    "for cluster in range(3):\n",
    "    plt.scatter(rfmls_sample[rfmls_sample['Cluster'] == cluster]['F'], rfmls_sample[rfmls_sample['Cluster'] == cluster]['M'], label=f'Cluster {cluster}')\n",
    "plt.xlabel('Fréquence')\n",
    "plt.ylabel('Monetary')\n",
    "plt.title('Fréquence vs Monetary')\n",
    "plt.legend()\n",
    "\n",
    "# Graphique Récence vs Monetary\n",
    "plt.subplot(1, 3, 3)\n",
    "for cluster in range(3):\n",
    "    plt.scatter(rfmls_sample[rfmls_sample['Cluster'] == cluster]['R'], rfmls_sample[rfmls_sample['Cluster'] == cluster]['M'], label=f'Cluster {cluster}')\n",
    "plt.xlabel('Récence')\n",
    "plt.ylabel('Monetary')\n",
    "plt.title('Récence vs Monetary')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vue 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Supposons que 'data' est votre DataFrame RFM avec une colonne 'Cluster'\n",
    "# data = pd.DataFrame(...)\n",
    "\n",
    "# Créer un graphique 3D avec Plotly\n",
    "fig = px.scatter_3d(rfmls_sample, x='R', y='F', z='M', color='Cluster',\n",
    "                    labels={'R': 'Récence', 'F': 'Fréquence', 'M': 'Monetary', 'Cluster': 'Cluster'},\n",
    "                    title='Clustering RFM en 3D')\n",
    "\n",
    "# Afficher le graphique\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement du modèle selectioné"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrainement Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list['kmeans']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sauvegarde du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maintenace des Modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, \n",
    "    davies_bouldin_score, \n",
    "    adjusted_rand_score, \n",
    "    calinski_harabasz_score  # Ajout de cette métrique\n",
    ")\n",
    "\n",
    "def compute_and_store_rfm(dfs, scaler, initial_model, product_familly_res, period=\"M\"):\n",
    "    \"\"\"\n",
    "    Calcule et stocke les résultats RFM pour chaque période définie par 'period'.\n",
    "\n",
    "    Paramètres :\n",
    "        dfs (pd.DataFrame): DataFrame contenant les données avec les colonnes R, F, M et order_purchase_timestamp.\n",
    "        scaler: Scaler pour normaliser les données RFM.\n",
    "        initial_model: Modèle initial de clustering.\n",
    "        product_familly_res (list): Liste pour stocker les résultats.\n",
    "        period (str): Fréquence de découpage ('M' pour mois, 'W' pour semaine, 'Y' pour année).\n",
    "    \"\"\"\n",
    "    # Définir les bornes temporelles selon la fréquence choisie\n",
    "    min_date = dfs['order_purchase_timestamp'].min()\n",
    "    max_date = dfs['order_purchase_timestamp'].max()\n",
    "    date_range = pd.date_range(start=min_date, end=max_date, freq=period)\n",
    "\n",
    "    for i in range(len(date_range) - 1):\n",
    "        time_bound = date_range[i + 1]\n",
    "        print(f\"Traitement pour la période se terminant avant {time_bound}...\")\n",
    "\n",
    "        # Filtrer les données jusqu'à la date limite\n",
    "        df_period = dfs[dfs['order_purchase_timestamp'] < time_bound]\n",
    "\n",
    "        if df_period.empty:\n",
    "            print(f\"Période vide avant {time_bound}, sautée.\")\n",
    "            continue\n",
    "\n",
    "        # Extraire les colonnes R, F, M\n",
    "        rfm = df_period[['R', 'F', 'M']].copy()\n",
    "\n",
    "        # Standardiser les données RFM\n",
    "        rfm_scaled = scaler.transform(rfm)\n",
    "\n",
    "        # Appliquer le modèle initial\n",
    "        labels_before_fit = initial_model.predict(rfm_scaled)\n",
    "        silh_before_fit = silhouette_score(rfm_scaled, labels_before_fit)\n",
    "        db_before_fit = davies_bouldin_score(rfm_scaled, labels_before_fit)\n",
    "        ch_before_fit = calinski_harabasz_score(rfm_scaled, labels_before_fit)  # Ajout ici\n",
    "\n",
    "        # Ré-entraîner le modèle K-means\n",
    "        fitted_model = KMeans(n_clusters=3, random_state=42).fit(rfm_scaled)\n",
    "        labels_after_fit = fitted_model.predict(rfm_scaled)\n",
    "        silh_after_fit = silhouette_score(rfm_scaled, labels_after_fit)\n",
    "        db_after_fit = davies_bouldin_score(rfm_scaled, labels_after_fit)\n",
    "        ch_after_fit = calinski_harabasz_score(rfm_scaled, labels_after_fit)  # Ajout ici\n",
    "\n",
    "        # Calculer l'indice de Rand ajusté\n",
    "        ari = adjusted_rand_score(labels_before_fit, labels_after_fit)\n",
    "\n",
    "        # Stocker les résultats\n",
    "        month_res = {\n",
    "            'time_bound': time_bound,\n",
    "            'dataframe': rfm,\n",
    "            'prop_data_period': round((rfm.shape[0] / dfs.shape[0] * 100), 2),\n",
    "            'labels_before_fit': labels_before_fit,\n",
    "            'silh_before_fit': silh_before_fit,\n",
    "            'db_before_fit': db_before_fit,\n",
    "            'ch_before_fit': ch_before_fit,  # Ajout ici\n",
    "            'fitted_model': fitted_model,\n",
    "            'labels_after_fit': labels_after_fit,\n",
    "            'silh_after_fit': silh_after_fit,\n",
    "            'db_after_fit': db_after_fit,\n",
    "            'ch_after_fit': ch_after_fit,  # Ajout ici\n",
    "            'ari': ari\n",
    "        }\n",
    "        product_familly_res.append(month_res)\n",
    "\n",
    "        # Afficher les résultats pour la période\n",
    "        print(f\"Période se terminant avant {time_bound}:\")\n",
    "        print(f\"Proportion du jeu de données: {month_res['prop_data_period']}%\")\n",
    "        print(f\"Indice de silhouette avant réentraînement: {silh_before_fit}\")\n",
    "        print(f\"Indice Davies-Bouldin avant réentraînement: {db_before_fit}\")\n",
    "        print(f\"Indice Calinski-Harabasz avant réentraînement: {ch_before_fit}\")  # Ajout ici\n",
    "        print(f\"Indice de silhouette après réentraînement: {silh_after_fit}\")\n",
    "        print(f\"Indice Davies-Bouldin après réentraînement: {db_after_fit}\")\n",
    "        print(f\"Indice Calinski-Harabasz après réentraînement: {ch_after_fit}\")  # Ajout ici\n",
    "        print(f\"Indice Rand ajusté: {ari}\")\n",
    "        print(\"-\" * 150)\n",
    "        print()\n",
    "\n",
    "\n",
    "def calculate_ari_timelaps(product_familly_res):\n",
    "    \"\"\"\n",
    "    Trace l'évolution de l'Indice Rand Ajusté (ARI) au cours du temps.\n",
    "\n",
    "    Paramètres :\n",
    "        product_familly_res (list): Liste contenant les résultats pour chaque période.\n",
    "    \"\"\"\n",
    "    # Extraire les dates et les valeurs ARI\n",
    "    dates = [res['time_bound'] for res in product_familly_res]\n",
    "    ari_values = [res['ari'] for res in product_familly_res]\n",
    "\n",
    "    # Tracer l'évolution de l'ARI\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(dates, ari_values, marker='o', linestyle='-')\n",
    "    plt.title('Évolution de l\\'Indice Rand Ajusté (ARI) au cours du temps')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('ARI')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exemple d'utilisation\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Initialisation des résultats\n",
    "#     product_familly_res = []\n",
    "\n",
    "#     # Charger votre DataFrame (remplacez ceci par votre propre code de chargement)\n",
    "#     # dfs = pd.read_csv(\"votre_fichier.csv\")\n",
    "#     # Assurez-vous que votre DataFrame contient les colonnes suivantes :\n",
    "#     # 'R', 'F', 'M', 'order_purchase_timestamp'\n",
    "\n",
    "#     # Exemple : Simuler un DataFrame\n",
    "#     data = {\n",
    "#         'customer_unique_id': [f'cust_{i}' for i in range(1000)],\n",
    "#         'R': [i % 365 for i in range(1000)],  # Recency\n",
    "#         'F': [i % 50 + 1 for i in range(1000)],  # Frequency\n",
    "#         'M': [i % 1000 + 100 for i in range(1000)],  # Monetary\n",
    "#         'order_purchase_timestamp': pd.date_range('2020-01-01', periods=1000, freq='D')\n",
    "#     }\n",
    "#     dfs = pd.DataFrame(data)\n",
    "\n",
    "#     # Normalisation des données RFM\n",
    "#     from sklearn.preprocessing import StandardScaler\n",
    "#     scaler = StandardScaler()\n",
    "\n",
    "#     # Initialiser le modèle de clustering initial\n",
    "#     initial_model = KMeans(n_clusters=3, random_state=42)\n",
    "\n",
    "#     # Découper automatiquement en périodes (par exemple, par mois)\n",
    "#     compute_and_store_rfm(dfs, scaler, initial_model, product_familly_res, period=\"M\")\n",
    "\n",
    "#     # Calculer et tracer l'évolution de l'ARI\n",
    "#     calculate_ari_timelaps(product_familly_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARI RFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link to sklearn doc [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html#sklearn.metrics.adjusted_rand_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfm_segment = pd.read_excel('RFM.xlsx')\n",
    "\n",
    "# Filter the dataset to include only relevant segments\n",
    "rfm_segment = rfm_segment[rfm_segment['Segment'].isin(['Perdus', 'À réactiver', 'À risque'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### labels_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the 'Segment' column into numerical labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb = LabelEncoder()\n",
    "true_labels = lb.fit_transform(rfm_segment['Segment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### labels_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10%  du df\n",
    "rfm_segment_sample_1 = rfm_segment.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Drop non-numeric columns and prepare the feature set\n",
    "rfm_segment_sample_1_predict = rfm_segment_sample_1.drop(\n",
    "    ['Segment', 'Unnamed: 0', 'customer_unique_id'], axis=1\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Use KMeans to predict cluster labels\n",
    "predicted_labels = model_list['kmeans'].fit_predict(rfm_segment_sample_1_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claculate ARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Calculate the Adjusted Rand Index\n",
    "ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "print(f\"Adjusted Rand Index: {ari}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ARI score /mois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des résultats\n",
    "product_familly_res = []\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Initialiser le modèle de clustering initial\n",
    "initial_model = KMeans(n_clusters=3, random_state=42)\n",
    "\n",
    "# Découper automatiquement en périodes (par exemple, par mois)\n",
    "compute_and_store_rfm(rfm_segment, scaler, initial_model, product_familly_res, period=\"M\")\n",
    "\n",
    "# Calculer et tracer l'évolution de l'ARI\n",
    "calculate_ari_timelaps(product_familly_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARI RFM LS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link to sklearn doc [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html#sklearn.metrics.adjusted_rand_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfm_segment = pd.read_excel('RFM.xlsx')\n",
    "\n",
    "# Filter the dataset to include only relevant segments\n",
    "rfm_segment = rfm_segment[rfm_segment['Segment'].isin(['Perdus', 'À réactiver', 'À risque'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### labels_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the 'Segment' column into numerical labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb = LabelEncoder()\n",
    "true_labels = lb.fit_transform(rfm_segment['Segment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### labels_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10%  du df\n",
    "rfm_segment_sample_1 = rfm_segment.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Drop non-numeric columns and prepare the feature set\n",
    "rfm_segment_sample_1_predict = rfm_segment_sample_1.drop(\n",
    "    ['Segment', 'Unnamed: 0', 'customer_unique_id'], axis=1\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Use KMeans to predict cluster labels\n",
    "predicted_labels = model_list['kmeans'].fit_predict(rfm_segment_sample_1_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Claculate ARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Calculate the Adjusted Rand Index\n",
    "ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "print(f\"Adjusted Rand Index: {ari}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ARI score /mois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_p5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
