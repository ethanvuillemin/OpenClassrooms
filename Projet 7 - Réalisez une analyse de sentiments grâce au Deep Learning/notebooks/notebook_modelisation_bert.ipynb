{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f52c3ab8",
      "metadata": {},
      "source": [
        "# <a id='toc1_'></a>[Modèle avancé BERT](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1163de3f",
      "metadata": {},
      "source": [
        "**Table of contents**<a id='toc0_'></a>    \n",
        "- [Modèle avancé BERT](#toc1_)    \n",
        "- [Telechargements & imports des données](#toc2_)    \n",
        "  - [Telechargement des libs](#toc2_1_)    \n",
        "  - [Import des données](#toc2_2_)    \n",
        "  - [Telechargement du dataset](#toc2_3_)    \n",
        "- [Preprocessing des données](#toc3_)    \n",
        "  - [Préprocessing simple](#toc3_1_)    \n",
        "  - [Tokenize and split data](#toc3_2_)    \n",
        "- [Modelisation](#toc4_)    \n",
        "  - [Train the model](#toc4_1_)    \n",
        "  - [Evaluate the model](#toc4_2_)    \n",
        "  - [Log the model with Mlflow](#toc4_3_)    \n",
        "  - [Save the model](#toc4_4_)    \n",
        "  - [Load and use the model](#toc4_5_)    \n",
        "\n",
        "<!-- vscode-jupyter-toc-config\n",
        "\tnumbering=false\n",
        "\tanchor=true\n",
        "\tflat=false\n",
        "\tminLevel=1\n",
        "\tmaxLevel=6\n",
        "\t/vscode-jupyter-toc-config -->\n",
        "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7c48d20",
      "metadata": {},
      "source": [
        "# <a id='toc2_'></a>[Telechargements & imports des données](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc2_1_'></a>[Telechargement des libs](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f26a441",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install uv\n",
        "# !uv pip install pandas numpy matplotlib scikit-learn wordcloud tqdm sentence_transformers ipykernel tensorflow spacy mlflow\n",
        "# !python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc2_2_'></a>[Import des données](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aad79e0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../models/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "\n",
        "import mlflow\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from mlflow.models.signature import infer_signature\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    average_precision_score,\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    roc_auc_score,\n",
        ")\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    BertTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "\n",
        "LOCAL = True\n",
        "\n",
        "if LOCAL:\n",
        "    MODEL_NAME = \"../models/bert-base-uncased\"\n",
        "    OUTPUT_DIR = \"./bert-base-uncased-trained\"\n",
        "else:\n",
        "    MODEL_NAME = \"bert-base-uncased\"\n",
        "    OUTPUT_DIR = \"/content/bert-base-uncased-trained\"\n",
        "\n",
        "\n",
        "# Load the pretrained model\n",
        "# Load BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "# Load pre-trained BERT for binary classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc2_3_'></a>[Telechargement du dataset](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4018d457",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Telecharger les données\n",
        "!wget https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/AI+Engineer/Project+7%C2%A0-+D%C3%A9tectez+les+Bad+Buzz+gr%C3%A2ce+au+Deep+Learning/sentiment140.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c3d6d34",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extraction des données\n",
        "ZIP_PATH = \"/content/sentiment140.zip\"\n",
        "!unzip $ZIP_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2665fa56",
      "metadata": {},
      "source": [
        "# <a id='toc3_'></a>[Preprocessing des données](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc3_1_'></a>[Préprocessing simple](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39959025",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "DATASET_PATH = \"../data/training.1600000.processed.noemoticon.csv\"\n",
        "df = pd.read_csv(DATASET_PATH, sep=\",\", encoding=\"ISO-8859-1\", header=None)\n",
        "\n",
        "# Rename the columns\n",
        "df = df.rename(\n",
        "    columns={\n",
        "        df.columns[0]: \"target\",\n",
        "        df.columns[1]: \"ids\",\n",
        "        df.columns[2]: \"date\",\n",
        "        df.columns[3]: \"flag\",\n",
        "        df.columns[4]: \"user\",\n",
        "        df.columns[5]: \"text\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# Define the datasets\n",
        "complete_df = df[[\"target\", \"text\"]]\n",
        "sample_df = df[[\"target\", \"text\"]].sample(16_000, random_state=42)\n",
        "\n",
        "# Convert to binary 0,1\n",
        "sample_df[\"target\"] = sample_df[\"target\"].replace({0: 0, 4: 1})\n",
        "complete_df[\"target\"] = complete_df[\"target\"].replace({0: 0, 4: 1})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23dc1304",
      "metadata": {},
      "outputs": [],
      "source": [
        "def tweet_cleaning(tweet):\n",
        "    \"\"\"\n",
        "    Nettoie et prétraite un tweet\n",
        "\n",
        "    Cette fonction effectue plusieurs étapes de nettoyage :\n",
        "        - Suppression des URLs, mentions et hashtags\n",
        "        - Suppression des emojis et caractères spéciaux\n",
        "        - Suppression de la ponctuation et des chiffres\n",
        "        - Normalisation du texte (minuscules, espaces multiples)\n",
        "\n",
        "    Params :\n",
        "        tweet (str) : Le tweet brut à nettoyer.\n",
        "\n",
        "    Return :\n",
        "        str : Le tweet nettoyé et prétraité, prêt pour l'analyse de sentiment.\n",
        "\n",
        "    \"\"\"\n",
        "    # Supprimer les URLs\n",
        "    tweet = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", tweet)\n",
        "\n",
        "    # Supprimer les mentions (@user)\n",
        "    tweet = re.sub(r\"@\\w+\", \"\", tweet)\n",
        "\n",
        "    # Supprimer les hashtags (#hashtag)\n",
        "    tweet = re.sub(r\"#\\w+\", \"\", tweet)\n",
        "\n",
        "    # Normaliser & supprimer les caractères\n",
        "    tweet = tweet.encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
        "    tweet = re.sub(r\"[^\\x00-\\x7F]+\", \"\", tweet)\n",
        "\n",
        "    # Supprimer la ponctuation\n",
        "    tweet = tweet.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    # Supprimer les chiffres\n",
        "    tweet = re.sub(r\"\\d+\", \"\", tweet)\n",
        "\n",
        "    # Supprimer les espaces multiples et les espaces au début/fin\n",
        "    tweet = re.sub(r\"\\s+\", \" \", tweet).strip()\n",
        "\n",
        "    return tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ce2fad9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# appliquer la fonction a la colonne text\n",
        "sample_df.apply(lambda x: tweet_cleaning(x[\"text\"]), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc3_2_'></a>[Tokenize and split data](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58d0c0e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_function(data):\n",
        "    \"\"\"Tokenise une phrase données avec le tokenizer bert\"\"\"\n",
        "    return tokenizer(\n",
        "        data[\"text\"], padding=\"max_length\", truncation=True, max_length=128\n",
        "    )\n",
        "\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_data = sample_df.apply(lambda x: preprocess_function(x), axis=1)\n",
        "\n",
        "# Convert tokenized_data to a Hugging Face Dataset\n",
        "tokenized_df = pd.DataFrame(\n",
        "    {\n",
        "        \"input_ids\": [data[\"input_ids\"] for data in tokenized_data],\n",
        "        \"attention_mask\": [data[\"attention_mask\"] for data in tokenized_data],\n",
        "        \"labels\": sample_df[\"target\"].tolist(),\n",
        "    }\n",
        ")\n",
        "\n",
        "# Creation du dataset huggingface\n",
        "dataset = Dataset.from_pandas(tokenized_df)\n",
        "\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "split_dataset = dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "# Get the training and test datasets\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "test_dataset = split_dataset[\"test\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "025d2f16",
      "metadata": {},
      "source": [
        "# <a id='toc4_'></a>[Modelisation](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc4_1_'></a>[Train the model](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "283d1cd6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    report_to=\"none\",\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,  # L2 regularization\n",
        "    save_total_limit=2,  # Save only the last 2 checkpoints\n",
        ")\n",
        "\n",
        "# Create the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc4_2_'></a>[Evaluate the model](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9798d315",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "predictions = trainer.predict(test_dataset)\n",
        "preds = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(test_dataset[\"labels\"], preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc4_3_'></a>[Log the model with Mlflow](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5139f30f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def log_huggingface_model_with_mlflow(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    tags,\n",
        "    model_name,\n",
        "    model_version=None,\n",
        "    experiment_name=\" Model Bert\",\n",
        "    hyperparams=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Enregistre un modèle Hugging Face avec MLflow.\n",
        "\n",
        "    Args:\n",
        "        model: Modèle Hugging Face à logger.\n",
        "        tokenizer: Tokenizer associé au modèle.\n",
        "        X_test: Données de test pour l'évaluation du modèle.\n",
        "        y_test: Labels de test pour l'évaluation du modèle.\n",
        "        tags (dict): Dictionnaire de tags supplémentaires.\n",
        "        model_name (str): Nom du modèle.\n",
        "        model_version (str, optional): Version du modèle.\n",
        "        experiment_name (str, optional): Nom de l'expérience MLflow.\n",
        "        hyperparams (dict, optional): Hyperparamètres du modèle.\n",
        "    \"\"\"\n",
        "    # Désactiver le logging automatique de MLflow pour les modèles Hugging Face\n",
        "    mlflow.transformers.autolog(disable=True)\n",
        "\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "\n",
        "    with mlflow.start_run(run_name=model_name):\n",
        "        # Récupération des hyperparamètres\n",
        "        if hyperparams is None:\n",
        "            hyperparams = {}\n",
        "            try:\n",
        "                # Pour les modèles Hugging Face, on peut essayer d'extraire certaines informations\n",
        "                hyperparams[\"model_type\"] = model.config.model_type\n",
        "                hyperparams[\"hidden_size\"] = model.config.hidden_size\n",
        "                hyperparams[\"num_hidden_layers\"] = model.config.num_hidden_layers\n",
        "                hyperparams[\"num_attention_heads\"] = model.config.num_attention_heads\n",
        "            except Exception as e:\n",
        "                print(\n",
        "                    f\"Impossible de récupérer les hyperparamètres automatiquement : {e}\"\n",
        "                )\n",
        "\n",
        "        for key, value in hyperparams.items():\n",
        "            mlflow.log_param(key, str(value))\n",
        "\n",
        "        # Évaluation du modèle\n",
        "        def evaluate_model(model, tokenizer, texts, labels):\n",
        "            # Tokenize les données de test\n",
        "            inputs = tokenizer(\n",
        "                texts,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "                max_length=128,\n",
        "            )\n",
        "\n",
        "            # Prédictions\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            # Obtenir les prédictions et probabilités\n",
        "            logits = outputs.logits\n",
        "            y_pred_proba = torch.softmax(logits, dim=1).numpy()[\n",
        "                :, 1\n",
        "            ]  # Probabilités pour la classe positive\n",
        "            y_pred_class = np.argmax(logits.numpy(), axis=1)\n",
        "\n",
        "            return y_pred_proba, y_pred_class\n",
        "\n",
        "        y_pred_proba, y_pred_class = evaluate_model(model, tokenizer, X_test, y_test)\n",
        "\n",
        "        # Calcul des métriques\n",
        "        accuracy = accuracy_score(y_test, y_pred_class)\n",
        "        precision = precision_score(y_test, y_pred_class)\n",
        "        recall = recall_score(y_test, y_pred_class)\n",
        "        f1 = f1_score(y_test, y_pred_class)\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "        pr_auc = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "        # Log des métriques dans MLflow\n",
        "        mlflow.log_metric(\"accuracy\", accuracy)\n",
        "        mlflow.log_metric(\"precision\", precision)\n",
        "        mlflow.log_metric(\"recall\", recall)\n",
        "        mlflow.log_metric(\"f1\", f1)\n",
        "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
        "        mlflow.log_metric(\"pr_auc\", pr_auc)\n",
        "\n",
        "        # Informations sur le modèle\n",
        "        mlflow.set_tag(\"mlflow.note.content\", model_name)\n",
        "        if model_version:\n",
        "            mlflow.set_tag(\"model_version\", model_version)\n",
        "\n",
        "        # Logger le modèle Hugging Face\n",
        "        # Créer un exemple d'entrée pour la signature\n",
        "        example_input = X_test[:1]  # Prendre un exemple des données de test\n",
        "\n",
        "        # Créer une signature pour le modèle\n",
        "        def predict_wrapper(texts):\n",
        "            inputs = tokenizer(\n",
        "                texts,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "                max_length=128,\n",
        "            )\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "            return torch.softmax(outputs.logits, dim=1).numpy()\n",
        "\n",
        "        # Inférer la signature à partir de l'exemple\n",
        "        signature = infer_signature(example_input, predict_wrapper(example_input))\n",
        "\n",
        "        # Logger le modèle avec MLflow\n",
        "        mlflow.transformers.log_model(\n",
        "            transformers_model={\"model\": model, \"tokenizer\": tokenizer},\n",
        "            artifact_path=\"model\",\n",
        "            signature=signature,\n",
        "            input_example=example_input,\n",
        "        )\n",
        "\n",
        "        # Ajouter les tags supplémentaires\n",
        "        for key, val in tags.items():\n",
        "            mlflow.set_tag(key, val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d96f3495",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logger le modele dans hugging face\n",
        "log_huggingface_model_with_mlflow(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    X_test=test_dataset[\"text\"],  # Assurez-vous que c'est une liste de textes\n",
        "    y_test=test_dataset[\"labels\"],  # Assurez-vous que c'est une liste de labels\n",
        "    tags={\n",
        "        \"dataset_used\": \"sentiment140\",\n",
        "        \"embedding_method\": \"Bert embedding\",\n",
        "        \"preprocessing\": \"tweet_cleaning_function\",\n",
        "        \"sample_size\": str(sample_df.shape[0]),\n",
        "        \"sample_seed\": \"42\",\n",
        "        },\n",
        "    \n",
        "    model_name=\"bert-base-uncased\",\n",
        "    model_version=\"1.0\",\n",
        "    hyperparams={\n",
        "        \"learning_rate\": 2e-5,\n",
        "        \"batch_size\": 16,\n",
        "        \"epochs\": 3\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc4_4_'></a>[Save the model](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b1ceb6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sauvegarder le modèle\n",
        "MODEL_SAVE_PATH = \"./bert\"\n",
        "model.save_pretrained(MODEL_SAVE_PATH)\n",
        "\n",
        "# Sauvegarder le tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"../models/bert-base-uncased\")\n",
        "tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
        "\n",
        "# Sauvegarder le modèle au format PyTorch\n",
        "torch.save(model.state_dict(), \"bert_model.pth\")\n",
        "\n",
        "print(f\"Modèle sauvegardé dans le dossier: {MODEL_SAVE_PATH}\")\n",
        "print(\"Modèle sauvegardé au format PyTorch: bert_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc4_5_'></a>[Load and use the model](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19c33527",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "# Charger le modèle\n",
        "model = BertForSequenceClassification.from_pretrained(MODEL_SAVE_PATH)\n",
        "model.load_state_dict(torch.load(\"bert_model.pth\"))\n",
        "\n",
        "# Charger le tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_SAVE_PATH)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv_lstm4",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
