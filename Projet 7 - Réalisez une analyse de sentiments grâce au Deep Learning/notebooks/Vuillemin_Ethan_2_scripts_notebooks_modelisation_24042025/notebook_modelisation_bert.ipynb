{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db34bcde",
   "metadata": {},
   "source": [
    "# Modèle avancé BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telechargements & imports des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b4e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install uv\n",
    "!uv pip install pandas numpy matplotlib scikit-learn wordcloud tqdm sentence_transformers ipykernel tensorflow spacy mlflow\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ef74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os, re, string\n",
    "\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e0b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Telecharger les données\n",
    "!wget https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/AI+Engineer/Project+7%C2%A0-+D%C3%A9tectez+les+Bad+Buzz+gr%C3%A2ce+au+Deep+Learning/sentiment140.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73dc658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des données\n",
    "ZIP_PATH = '/content/sentiment140.zip'\n",
    "\n",
    "!unzip $ZIP_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126c7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du Dataframe\n",
    "DATASET_PATH = '/content/training.1600000.processed.noemoticon.csv'\n",
    "df = pd.read_csv(DATASET_PATH, sep=',', encoding = \"ISO-8859-1\", header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0dd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renommer les colonnes en ce basant sur les cards du dataset\n",
    "df = df.rename(columns={\n",
    "    df.columns[0]: 'target',\n",
    "    df.columns[1]: 'ids',\n",
    "    df.columns[2]: 'date',\n",
    "    df.columns[3]: 'flag',\n",
    "    df.columns[4]: 'user',\n",
    "    df.columns[5]: 'text',\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81405b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir les jeux de données\n",
    "\n",
    "complete_df = df[['target', 'text']]\n",
    "sample_df = df[['target', 'text']].sample(16_000)\n",
    "\n",
    "# Afficher la valeurs des labels initiaux\n",
    "print(sample_df['target'].value_counts())\n",
    "\n",
    "# Conversion en binaire 0,1\n",
    "sample_df['target'] = sample_df['target'].replace({0: 0, 4: 1})\n",
    "complete_df['target'] = complete_df['target'].replace({0: 0, 4: 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a9da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tweet_cleaning(tweet):\n",
    "    \"\"\"\n",
    "    Nettoie et prétraite un tweet\n",
    "\n",
    "    Cette fonction effectue plusieurs étapes de nettoyage :\n",
    "        - Suppression des URLs, mentions et hashtags\n",
    "        - Suppression des emojis et caractères spéciaux\n",
    "        - Suppression de la ponctuation et des chiffres\n",
    "        - Normalisation du texte (minuscules, espaces multiples)\n",
    "\n",
    "    Params :\n",
    "        tweet (str) : Le tweet brut à nettoyer.\n",
    "\n",
    "    Return :\n",
    "        str : Le tweet nettoyé et prétraité, prêt pour l'analyse de sentiment.\n",
    "\n",
    "    \"\"\"\n",
    "    # Supprimer les URLs\n",
    "    tweet = re.sub(r'https?://\\S+|www\\.\\S+', '', tweet)\n",
    "\n",
    "    # Supprimer les mentions (@user)\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "\n",
    "    # Supprimer les hashtags (#hashtag)\n",
    "    tweet = re.sub(r'#\\w+', '', tweet)\n",
    "\n",
    "    # Normaliser & supprimer les caractères\n",
    "    tweet = tweet.encode('ascii', 'ignore').decode('utf-8')\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+', '', tweet)\n",
    "\n",
    "    # Supprimer la ponctuation\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Supprimer les chiffres\n",
    "    tweet = re.sub(r'\\d+', '', tweet)\n",
    "\n",
    "    # Supprimer les espaces multiples et les espaces au début/fin\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "\n",
    "    return tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e9583",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.apply(lambda x: tweet_cleaning(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample_df['text'].apply(tweet_cleaning)\n",
    "y = sample_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570de4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_test, X_val, y_test, y_val  = train_test_split(X_test, y_test, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb2caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MAX_LENGTH = 16\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 6\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "def encode_texts(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors=\"tf\",\n",
    "    )\n",
    "\n",
    "\n",
    "train_encodings = encode_texts(X_train)\n",
    "val_encodings = encode_texts(X_val)\n",
    "test_encodings = encode_texts(X_test)\n",
    "\n",
    "# Conversion en tf.data.Dataset\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train))\n",
    "    .shuffle(len(X_train))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((dict(val_encodings), y_val))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "print(\"\\nExemple d'encodage (première phrase d'entraînement):\")\n",
    "for key, value in train_encodings.items():\n",
    "    print(f\"{key}: {value[0].numpy().tolist()[:10]}...\")  # Affiche les 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation modèle BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf58108",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=2e-5)\n",
    "loss = SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "# Prepare TensorFlow datasets\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train))\n",
    "    .shuffle(1000)\n",
    "    .batch(16)\n",
    ")\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(\n",
    "    16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95cc59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=EPOCHS, validation_data=val_dataset)\n",
    "results = model.evaluate(test_dataset, batch_size=BATCH_SIZE, return_dict=True)\n",
    "\n",
    "\n",
    "print(f\"Résultats du test: {results}\")\n",
    "mlflow.log_metrics(\n",
    "    {\"test_loss\": results[\"loss\"], \"test_accuracy\": results[\"accuracy\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fff771",
   "metadata": {},
   "source": [
    "## Evaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d948e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization des exemples\n",
    "sample_encodings = encode_texts(X_test)\n",
    "\n",
    "# Prédictions \n",
    "predictions = model.predict(dict(sample_encodings))\n",
    "logits = predictions.logits\n",
    "\n",
    "# Conversion des logits en probabilités et en classes prédites\n",
    "probabilities = tf.nn.softmax(logits, axis=-1).numpy()\n",
    "predicted_classes = np.argmax(probabilities, axis=1)\n",
    "\n",
    "for tweet, true, prob, pred_class in zip(\n",
    "    X_test, y_test, probabilities, predicted_classes\n",
    ")[:50]:\n",
    "    sentiment = \"Non-Négatif/Positif\" if pred_class == 1 else \"Négatif\"\n",
    "    print(f\"\\nTweet: {tweet}\")\n",
    "    print(f\"  Probabilités (Négatif, Non-Négatif/Positif): {prob}\")\n",
    "    print(f\"  Sentiment Prédit: {sentiment} | 'Vrai' Sentiment : {true}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
